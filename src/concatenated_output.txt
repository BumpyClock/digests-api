//getreaderview.go
// Package main provides the main functionality for the web server.
package main

import (
	"encoding/json"
	"fmt"
	"net/http"
	"sync"
	"time"

	"github.com/sirupsen/logrus"

	readability "github.com/go-shiori/go-readability"
)

/**
 * @function getReaderViewResult
 * @description Retrieves the reader view of a given URL using the go-readability library.
 *              It returns a ReaderViewResult containing the parsed content or an error.
 * @param {string} url The URL to retrieve the reader view for.
 * @returns {ReaderViewResult} A struct containing the reader view result or an error.
 * @dependencies getReaderView, log
 */
func getReaderViewResult(url string) ReaderViewResult {
	readerView, err := getReaderView(url)
	if err != nil {
		log.WithFields(logrus.Fields{
			"url":   url,
			"error": err,
		}).Error("[ReaderView] Error retrieving content")
		return ReaderViewResult{
			URL:    url,
			Status: "error",
			Error:  err,
		}
	}
	return ReaderViewResult{
		URL:         url,
		Status:      "ok",
		ReaderView:  readerView.Content,
		Title:       readerView.Title,
		SiteName:    readerView.SiteName,
		Image:       readerView.Image,
		Favicon:     readerView.Favicon,
		TextContent: readerView.TextContent,
	}
}

/**
 * @function getReaderViewHandler
 * @description Handles HTTP requests to the /getreaderview endpoint.
 *              It expects a POST request with a JSON body containing an array of URLs.
 *              It retrieves the reader view for each URL, caches the results, and returns a JSON response.
 * @param {http.ResponseWriter} w The HTTP response writer.
 * @param {*http.Request} r The HTTP request.
 * @returns {void}
 * @dependencies getReaderViewResult, cache, createHash, log
 */
func getReaderViewHandler(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		log.WithFields(logrus.Fields{
			"method": r.Method,
		}).Warn("[ReaderView] Invalid method")
		http.Error(w, "Bad Request", http.StatusBadRequest)
		return
	}

	var urls Urls
	if err := json.NewDecoder(r.Body).Decode(&urls); err != nil {
		log.WithFields(logrus.Fields{
			"error": err,
		}).Error("[ReaderView] Error decoding request body")
		http.Error(w, "Bad Request", http.StatusBadRequest)
		return
	}

	var wg sync.WaitGroup
	results := make([]ReaderViewResult, len(urls.Urls))

	for i, url := range urls.Urls {
		wg.Add(1)
		go func(i int, url string) {
			defer wg.Done()

			cacheKey := createHash(url)
			var result ReaderViewResult

			if err := cache.Get(readerView_prefix, cacheKey, &result); err != nil {
				log.WithFields(logrus.Fields{
					"url": url,
				}).Info("[ReaderView] Cache miss")
				result = getReaderViewResult(url)
				if len(result.TextContent) < 100 {
					result.TextContent = `<div id="readability-page-1" class="page"><p id="cmsg">Error getting reader view or site requires subscription. Please open the link in a new tab.</p></div>`
					result.ReaderView = result.TextContent
				}
				if err := cache.Set(readerView_prefix, cacheKey, result, 24*time.Hour); err != nil {
					log.WithFields(logrus.Fields{
						"url":   url,
						"error": err,
					}).Error("[ReaderView] Failed to cache reader view")
				}
			} else {
				log.WithFields(logrus.Fields{
					"url": url,
				}).Info("[ReaderView] Cache hit")
			}
			results[i] = result
		}(i, url)
	}
	wg.Wait()

	w.Header().Set("Content-Type", "application/json")
	if err := json.NewEncoder(w).Encode(results); err != nil {
		log.WithFields(logrus.Fields{
			"error": err,
		}).Error("[ReaderView] Failed to encode JSON")
		http.Error(w, "Internal Server Error", http.StatusInternalServerError)
	}
}

/**
 * @function getReaderView
 * @description Retrieves the reader view of a given URL using the go-readability library.
 *              It sets a timeout for the request and returns the parsed article or an error.
 * @param {string} url The URL to retrieve the reader view for.
 * @returns {readability.Article, error} The parsed article or an error.
 * @dependencies readability.FromURL, log
 */
func getReaderView(url string) (readability.Article, error) {
	article, err := readability.FromURL(url, 30*time.Second)
	if err != nil {
		log.WithFields(logrus.Fields{
			"url":   url,
			"error": err,
		}).Error("[ReaderView] Failed to parse URL")
		return readability.Article{}, fmt.Errorf("failed to parse URL: %w", err)
	}
	return article, nil
}

//server.go
package main

import (
	"compress/gzip"
	"flag"
	"net/http"
	_ "net/http/pprof"
	"runtime"
	"strings"
	"sync"
	"time"

	digestsCache "digests-app-api/cache"

	"github.com/rs/cors"
	"github.com/sirupsen/logrus"
	"golang.org/x/time/rate"
)

var (
	limiter       = rate.NewLimiter(5, 15) // e.g., 5 requests/sec, burst 15
	cache         digestsCache.Cache
	log           = logrus.New()
	urlList       []string
	urlListMutex  = &sync.Mutex{}
	refresh_timer = 60
	redis_address = "localhost:6379"
	numWorkers    = runtime.NumCPU()
	cacheMutex    = &sync.Mutex{}
	httpClient    = &http.Client{Timeout: 20 * time.Second}
	cachePeriod   = 30
)

func main() {
	// Start pprof profiling in a goroutine
	go func() {
		log.Println(http.ListenAndServe("localhost:6060", nil))
	}()

	port := flag.String("port", "8000", "port to run the application on")
	timer := flag.Int("timer", refresh_timer, "timer to refresh the cache")
	redis := flag.String("redis", "localhost:6379", "redis address")
	flag.Parse()

	mux := http.NewServeMux()
	log.Infof("Number of workers: %v", numWorkers)

	// Setup routes
	InitializeRoutes(mux)

	// Wrap mux with middlewares
	var handler http.Handler = mux

	// 1) Recover from panics
	handler = errorRecoveryMiddleware(handler)

	// 2) CORS
	handler = cors.New(cors.Options{
		AllowedOrigins:   []string{"*"},
		AllowedMethods:   []string{http.MethodGet, http.MethodPost, http.MethodOptions, http.MethodPut, http.MethodDelete},
		AllowedHeaders:   []string{"Accept", "Content-Type", "Content-Length", "Accept-Encoding", "X-CSRF-Token", "Authorization"},
		AllowCredentials: true,
	}).Handler(handler)

	// 3) Rate limit
	handler = RateLimitMiddleware(handler)

	// 4) GZIP compression
	handler = GzipMiddleware(handler)

	// Cache setup
	redis_address = *redis
	log.Info("Opening cache connection...")

	redisCache, redisErr := digestsCache.NewRedisCache(redis_address, redis_password, redis_db)
	if redisErr != nil {
		log.Warnf("Failed to open Redis cache (%v); falling back to in-memory cache", redisErr)
		cache = digestsCache.NewGoCache(5*time.Minute, 10*time.Minute)
	} else {
		cache = redisCache
	}

	cachesize, cacheerr := cache.Count()
	if cacheerr != nil {
		log.Errorf("Failed to get cache size: %v", cacheerr)
	} else {
		log.Infof("Cache has %d items", cachesize)
	}

	// Set refresh timer from command-line
	refresh_timer = *timer
	refreshFeeds()

	// Periodic refresh
	go func() {
		ticker := time.NewTicker(time.Duration(refresh_timer*4) * time.Minute)
		defer ticker.Stop()

		for range ticker.C {
			log.Info("Refreshing cache (periodic)...")
			refreshFeeds()
			log.Infof("Cache refreshed at %v, urlList=%v", time.Now().Format(time.RFC3339), urlList)
		}
	}()

	log.Infof("Server is starting on port %v", *port)
	log.Infof("Refresh timer is %v minutes", refresh_timer)
	log.Infof("Redis address is %v", redis_address)

	err := http.ListenAndServe(":"+*port, handler)
	if err != nil {
		log.Fatalf("Server failed to start: %v", err)
	}
}

// errorRecoveryMiddleware ensures the server recovers from unexpected panics
func errorRecoveryMiddleware(next http.Handler) http.Handler {
	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		defer func() {
			if rec := recover(); rec != nil {
				log.Errorf("Panic recovered: %v", rec)
				http.Error(w, "Internal Server Error", http.StatusInternalServerError)
			}
		}()
		next.ServeHTTP(w, r)
	})
}

// GzipMiddleware adds gzip compression
func GzipMiddleware(next http.Handler) http.Handler {
	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		if !strings.Contains(r.Header.Get("Accept-Encoding"), "gzip") {
			next.ServeHTTP(w, r)
			return
		}

		wrw := gzipResponseWriter{ResponseWriter: w}
		wrw.Header().Set("Content-Encoding", "gzip")
		defer wrw.Flush()

		next.ServeHTTP(&wrw, r)
	})
}

type gzipResponseWriter struct {
	http.ResponseWriter
	wroteHeader bool
	writer      *gzip.Writer
}

func (w *gzipResponseWriter) Write(b []byte) (int, error) {
	if !w.wroteHeader {
		w.Header().Del("Content-Length")
		w.writer = gzip.NewWriter(w.ResponseWriter)
		w.wroteHeader = true
	}
	return w.writer.Write(b)
}

func (w *gzipResponseWriter) WriteHeader(status int) {
	w.ResponseWriter.WriteHeader(status)
	if w.wroteHeader && w.writer != nil {
		w.writer.Close()
	}
}

func (w *gzipResponseWriter) Flush() {
	if w.wroteHeader {
		_ = w.writer.Flush()
	}
	if flusher, ok := w.ResponseWriter.(http.Flusher); ok {
		flusher.Flush()
	}
}

// RateLimitMiddleware applies a rate limiter
func RateLimitMiddleware(next http.Handler) http.Handler {
	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		if !limiter.Allow() {
			http.Error(w, "Too Many Requests", http.StatusTooManyRequests)
			return
		}
		next.ServeHTTP(w, r)
	})
}

//sqliteCache.go
// Package digestsCache provides caching implementations.
package digestsCache

import (
	"database/sql"
	"encoding/json"
	"errors"
	"time"

	_ "github.com/mattn/go-sqlite3" // SQLite driver
	"github.com/sirupsen/logrus"
)

// SQLiteCache implements the Cache interface using a local SQLite database.
type SQLiteCache struct {
	db *sql.DB
}

var ErrSQLCacheMiss = errors.New("sqlite cache: key not found or expired")

// NewSQLiteCache creates a new SQLiteCache instance and initializes the database schema.
func NewSQLiteCache(dbPath string) (*SQLiteCache, error) {
	db, err := sql.Open("sqlite3", dbPath)
	if err != nil {
		logrus.WithField("dbPath", dbPath).Error("Failed to open SQLite database")
		return nil, err
	}

	// Enable WAL mode for better performance
	if _, err := db.Exec("PRAGMA journal_mode=WAL;"); err != nil {
		logrus.WithError(err).Error("Failed to enable WAL mode")
	}

	// Create the cache table if it doesn't exist
	schema := `
	CREATE TABLE IF NOT EXISTS cache (
		id TEXT PRIMARY KEY,
		data TEXT NOT NULL,
		expiration INTEGER NOT NULL
	);
	CREATE INDEX IF NOT EXISTS idx_expiration ON cache (expiration);
	`
	if _, err := db.Exec(schema); err != nil {
		logrus.WithField("error", err).Error("Failed to create cache table in SQLite")
		return nil, err
	}

	return &SQLiteCache{db: db}, nil
}

// Set inserts or updates a key in the SQLite database with an expiration time.
// prefix:key is used as the primary key.
func (c *SQLiteCache) Set(prefix string, key string, value interface{}, expiration time.Duration) error {
	fullKey := prefix + ":" + key

	valBytes, err := json.Marshal(value)
	if err != nil {
		logrus.WithFields(logrus.Fields{
			"key": key,
		}).Error("Failed to marshal value for key (SQLite)")
		return err
	}

	expirationUnix := time.Now().Add(expiration).Unix()
	if expiration == 0 {
		// If expiration is 0, set a large expiration in the distant future
		expirationUnix = time.Now().AddDate(10, 0, 0).Unix()
	}

	// Use a transaction for better performance
	tx, err := c.db.Begin()
	if err != nil {
		logrus.WithError(err).Error("Failed to begin transaction")
		return err
	}
	defer tx.Rollback() // Rollback is safe to call even if the transaction was committed

	stmt := `
	INSERT INTO cache(id, data, expiration)
	VALUES(?, ?, ?)
	ON CONFLICT(id) DO UPDATE SET
		data=excluded.data,
		expiration=excluded.expiration;
	`

	_, err = tx.Exec(stmt, fullKey, valBytes, expirationUnix)
	if err != nil {
		logrus.WithFields(logrus.Fields{
			"key":   fullKey,
			"error": err,
		}).Error("Failed to set key in SQLite cache")
		return err
	}

	if err := tx.Commit(); err != nil {
		logrus.WithError(err).Error("Failed to commit transaction")
		return err
	}

	return nil
}

// Get retrieves a key from the SQLite database and unmarshals it into dest.
// If the key is expired or missing, it returns an error.
func (c *SQLiteCache) Get(prefix string, key string, dest interface{}) error {
	fullKey := prefix + ":" + key

	stmt := `
	SELECT data, expiration FROM cache
	WHERE id = ? AND expiration > ?
	LIMIT 1;
	`

	row := c.db.QueryRow(stmt, fullKey, time.Now().Unix())

	var (
		data       []byte
		expiration int64
	)
	err := row.Scan(&data, &expiration)
	if err != nil {
		if err == sql.ErrNoRows {
			logrus.WithFields(logrus.Fields{
				"key": fullKey,
			}).Debug("Key not found in SQLite cache")
			return ErrCacheMiss
		}
		logrus.WithFields(logrus.Fields{
			"key":   fullKey,
			"error": err,
		}).Error("Failed to read key from SQLite cache")
		return err
	}

	// Unmarshal data into dest
	if err := json.Unmarshal(data, dest); err != nil {
		logrus.WithFields(logrus.Fields{
			"key":   fullKey,
			"error": err,
		}).Error("Failed to unmarshal value from SQLite cache")
		return err
	}

	return nil
}

// GetSubscribedListsFromCache scans the cache table for records that start with prefix:
// and attempts to unmarshal them into a FeedItem to extract the FeedUrl.
func (c *SQLiteCache) GetSubscribedListsFromCache(prefix string) ([]string, error) {
	var urls []string

	stmt := `
	SELECT id, data FROM cache
	WHERE id LIKE ? AND expiration > ?;
	`
	rows, err := c.db.Query(stmt, prefix+":%", time.Now().Unix())
	if err != nil {
		logrus.WithFields(logrus.Fields{
			"error": err,
		}).Error("Failed to query SQLite cache for subscribed lists")
		return nil, err
	}
	defer rows.Close()

	for rows.Next() {
		var (
			fullKey string
			data    []byte
		)
		if err := rows.Scan(&fullKey, &data); err != nil {
			logrus.WithFields(logrus.Fields{
				"error": err,
			}).Error("Failed to scan SQLite cache row")
			continue
		}

		var feedItem FeedItem
		if err := json.Unmarshal(data, &feedItem); err != nil {
			logrus.WithFields(logrus.Fields{
				"key":   fullKey,
				"error": err,
			}).Error("Failed to unmarshal value from SQLite cache")
			continue
		}

		if feedItem.FeedUrl != "" {
			urls = append(urls, feedItem.FeedUrl)
		}
	}

	if err := rows.Err(); err != nil {
		logrus.WithFields(logrus.Fields{
			"error": err,
		}).Error("Error iterating SQLite cache rows")
		return nil, err
	}

	return urls, nil
}

// SetFeedItems fetches existing feed items from the cache, deduplicates them with newItems,
// then updates the cache with the merged slice.
func (c *SQLiteCache) SetFeedItems(prefix string, key string, newItems []FeedItem, expiration time.Duration) error {
	var existingItems []FeedItem
	err := c.Get(prefix, key, &existingItems)
	if err != nil && !errors.Is(err, ErrCacheMiss) && !errors.Is(err, ErrSQLCacheMiss) {
		return err
	}

	// Deduplicate
	itemMap := make(map[string]FeedItem)
	for _, item := range existingItems {
		itemMap[item.GUID] = item
	}
	for _, newItem := range newItems {
		itemMap[newItem.GUID] = newItem
	}

	uniqueItems := make([]FeedItem, 0, len(itemMap))
	for _, item := range itemMap {
		uniqueItems = append(uniqueItems, item)
	}

	return c.Set(prefix, key, uniqueItems, expiration)
}

// Count returns the total number of items in the cache (including expired items).
func (c *SQLiteCache) Count() (int64, error) {
	stmt := `SELECT COUNT(*) FROM cache;`
	var count int64
	err := c.db.QueryRow(stmt).Scan(&count)
	if err != nil {
		logrus.WithFields(logrus.Fields{
			"error": err,
		}).Error("Failed to count items in SQLite cache")
		return 0, err
	}
	return count, nil
}

//redisCache.go
// Package digestsCache provides caching implementations.
package digestsCache

import (
	"context"
	"encoding/json"
	"strings"
	"time"

	"github.com/nitishm/go-rejson/v4"
	"github.com/redis/go-redis/v9"
	"github.com/sirupsen/logrus"
)

var ctx = context.Background()
var log = logrus.New()

type RedisCache struct {
	client  *redis.Client
	handler *rejson.Handler
}

type FeedItem struct {
	GUID    string `json:"guid"`
	FeedUrl string `json:"feedUrl"`
	// Include other fields as necessary.
}

func NewRedisCache(addr string, password string, db int) (*RedisCache, error) {
	client := redis.NewClient(&redis.Options{
		Addr:     addr,
		Password: password,
		DB:       db,
	})

	_, err := client.Ping(context.Background()).Result()
	if err != nil {
		log.WithFields(logrus.Fields{
			"address":  addr,
			"database": db,
		}).Error("Failed to connect to Redis")
		return nil, err
	}

	handler := rejson.NewReJSONHandler()
	handler.SetGoRedisClient(client)

	return &RedisCache{client: client, handler: handler}, nil
}

func (cache *RedisCache) Set(prefix string, key string, value interface{}, expiration time.Duration) error {
	_, err := cache.handler.JSONSet(prefix+":"+key, ".", value)
	if err != nil {
		log.WithFields(logrus.Fields{
			"key": key,
		}).Error("Failed to set key in Redis")
		return err
	}

	if expiration != 0 {
		err = cache.client.Expire(ctx, prefix+":"+key, expiration).Err()
		if err != nil {
			log.WithFields(logrus.Fields{
				"key": key,
			}).Error("Failed to set expiration for key in Redis")
		}
	}

	return err
}

func (cache *RedisCache) Get(prefix string, key string, dest interface{}) error {
	val, err := cache.handler.JSONGet(prefix+":"+key, ".")
	if err != nil {
		if err == redis.Nil {
			log.WithFields(logrus.Fields{
				"key": key,
			}).Info("Key not found in Redis")
			return ErrCacheMiss // Use the common ErrCacheMiss
		}
		log.WithFields(logrus.Fields{
			"key": key,
		}).Error("Failed to get key from Redis")
		return err
	}

	// Convert val to []byte, then to string
	valStr := string(val.([]byte))

	err = json.Unmarshal([]byte(valStr), dest)
	if err != nil {
		log.WithFields(logrus.Fields{
			"key": key,
		}).Error("Failed to unmarshal value for key from Redis")
	}

	return err
}

func (cache *RedisCache) GetSubscribedListsFromCache(prefix string) ([]string, error) {
	ctx := context.Background()                               // Create a new context
	keys, err := cache.client.Keys(ctx, prefix+":*").Result() // Pass the context to the Keys method
	if err != nil {
		log.WithFields(logrus.Fields{
			"error": err,
		}).Error("Failed to get keys from Redis")
		return nil, err
	}

	var urls []string
	for _, key := range keys {
		var feedItem FeedItem
		actualKey := strings.TrimPrefix(key, prefix+":") // Remove the prefix from the key
		err := cache.Get(prefix, actualKey, &feedItem)
		if err != nil {
			log.WithFields(logrus.Fields{
				"key":   actualKey,
				"error": err,
			}).Error("Failed to get value from Redis")
			continue
		}

		if feedItem.FeedUrl != "" {
			urls = append(urls, feedItem.FeedUrl)
		}
	}

	return urls, nil
}

func (cache *RedisCache) SetFeedItems(prefix string, key string, newItems []FeedItem, expiration time.Duration) error {
	// Fetch existing items from cache
	var existingItems []FeedItem
	err := cache.Get(prefix, key, &existingItems)
	if err != nil && err != redis.Nil {
		return err
	}

	// Deduplication based on GUID
	itemMap := make(map[string]FeedItem)
	for _, item := range existingItems {
		itemMap[item.GUID] = item
	}
	for _, newItem := range newItems {
		itemMap[newItem.GUID] = newItem // This will replace existing items with the same GUID or add new ones
	}

	// Convert map back to slice
	uniqueItems := make([]FeedItem, 0, len(itemMap))
	for _, item := range itemMap {
		uniqueItems = append(uniqueItems, item)
	}

	// Cache the deduplicated slice of items
	return cache.Set(prefix, key, uniqueItems, expiration)
}

func (cache *RedisCache) Count() (int64, error) {
	return cache.client.DBSize(ctx).Result()
}

//goCache.go
// Package digestsCache provides caching implementations.
package digestsCache

import (
	"encoding/json"
	"errors"
	"strings"
	"time"

	"github.com/patrickmn/go-cache"
	"github.com/sirupsen/logrus"
)

type GoCache struct {
	cache *cache.Cache
}

var ErrCacheMiss = errors.New("cache: key not found")

func NewGoCache(defaultExpiration, cleanupInterval time.Duration) *GoCache {
	c := cache.New(defaultExpiration, cleanupInterval)
	return &GoCache{cache: c}
}

func (c *GoCache) Set(prefix string, key string, value interface{}, expiration time.Duration) error {
	fullKey := prefix + ":" + key
	valBytes, err := json.Marshal(value)
	if err != nil {
		log.WithFields(logrus.Fields{
			"key": key,
		}).Error("Failed to marshal value for key")
		return err
	}
	c.cache.Set(fullKey, valBytes, expiration)
	return nil
}

func (c *GoCache) Get(prefix string, key string, dest interface{}) error {
	fullKey := prefix + ":" + key
	val, found := c.cache.Get(fullKey)
	if !found {
		log.WithFields(logrus.Fields{
			"key": key,
		}).Info("Key not found in cache")
		return ErrCacheMiss
	}

	valBytes, ok := val.([]byte)
	if !ok {
		log.WithFields(logrus.Fields{
			"key": key,
		}).Error("Failed to assert type of cached value")
		return ErrCacheMiss // Or define a more appropriate error
	}

	err := json.Unmarshal(valBytes, dest)
	if err != nil {
		log.WithFields(logrus.Fields{
			"key": key,
		}).Error("Failed to unmarshal cached value")
		return err
	}

	return nil
}

func (c *GoCache) GetSubscribedListsFromCache(prefix string) ([]string, error) {
	var urls []string
	for k, v := range c.cache.Items() {
		if strings.HasPrefix(k, prefix+":") {
			var feedItem FeedItem
			err := json.Unmarshal(v.Object.([]byte), &feedItem)
			if err != nil {
				log.WithFields(logrus.Fields{
					"key":   k,
					"error": err,
				}).Error("Failed to unmarshal value from cache")
				continue
			}

			if feedItem.FeedUrl != "" {
				urls = append(urls, feedItem.FeedUrl)
			}
		}
	}
	return urls, nil
}

func (c *GoCache) SetFeedItems(prefix string, key string, newItems []FeedItem, expiration time.Duration) error {
	var existingItems []FeedItem
	err := c.Get(prefix, key, &existingItems)
	if err != nil && err != ErrCacheMiss {
		return err
	}

	itemMap := make(map[string]FeedItem)
	for _, item := range existingItems {
		itemMap[item.GUID] = item
	}
	for _, newItem := range newItems {
		itemMap[newItem.GUID] = newItem
	}

	uniqueItems := make([]FeedItem, 0, len(itemMap))
	for _, item := range itemMap {
		uniqueItems = append(uniqueItems, item)
	}

	return c.Set(prefix, key, uniqueItems, expiration)
}

func (c *GoCache) Count() (int64, error) {
	return int64(c.cache.ItemCount()), nil
}

//cacheInterface.go
// Package digestsCache provides caching implementations.
package digestsCache

import "time"

// Cache is the interface that defines the methods for a cache implementation.
type Cache interface {
	Set(prefix string, key string, value interface{}, expiration time.Duration) error
	Get(prefix string, key string, dest interface{}) error
	GetSubscribedListsFromCache(prefix string) ([]string, error)
	SetFeedItems(prefix string, key string, newItems []FeedItem, expiration time.Duration) error
	Count() (int64, error)
}

//dataModel.go
// Package models defines the data structures used in the application.
package main

import (
	"github.com/mmcdole/gofeed"
)

// ParseRequest represents the expected incoming JSON payload structure.
// ParseRequest represents the expected JSON body for the /parse endpoint.
type ParseRequest struct {
	URLs         []string `json:"urls"`
	Page         int      `json:"page"`
	ItemsPerPage int      `json:"itemsperpage"`
}

type RGBColor struct {
	R uint8 `json:"r"`
	G uint8 `json:"g"`
	B uint8 `json:"b"`
}

// Add these structs to your existing code
type MediaContent struct {
	URL    string `xml:"url,attr"`
	Width  int    `xml:"width,attr"`
	Height int    `xml:"height,attr"`
}

type ExtendedItem struct {
	*gofeed.Item
	MediaContent []MediaContent `xml:"http://search.yahoo.com/mrss/ content"`
}

// FeedResponseItem represents an enriched structure for an individual feed item.
type FeedResponseItem struct {
	Type                   string                      `json:"type"`
	ID                     string                      `json:"id"`
	Title                  string                      `json:"title"`
	Description            string                      `json:"description"`
	Link                   string                      `json:"link"`
	Author                 string                      `json:"author"`
	Published              string                      `json:"published"`
	Content                string                      `json:"content"`
	Created                string                      `json:"created"`
	Content_Encoded        string                      `json:"content_encoded"`
	Categories             string                      `json:"categories"`
	Enclosures             []*gofeed.Enclosure         `json:"enclosures"`
	Thumbnail              string                      `json:"thumbnail"`
	ThumbnailColor         RGBColor                    `json:"thumbnailColor"`
	ThumbnailColorComputed string                      `json:"thumbnailColorComputed"`
	EpisodeType            string                      `json:"episodeType,omitempty"`
	Subtitle               []PodcastTranscriptsDetails `json:"subtitle,omitempty"`
	Duration               int                         `json:"duration,omitempty"`
}

// FeedResponse represents the structure for the overall feed, including metadata and items.
type FeedResponse struct {
	Type          string              `json:"type"`
	GUID          string              `json:"guid"`
	Status        string              `json:"status"`
	Error         error               `json:"error,omitempty"`
	SiteTitle     string              `json:"siteTitle"`
	FeedTitle     string              `json:"feedTitle"`
	FeedUrl       string              `json:"feedUrl"`
	Description   string              `json:"description"`
	Link          string              `json:"link"`
	LastUpdated   string              `json:"lastUpdated"`
	LastRefreshed string              `json:"lastRefreshed"`
	Published     string              `json:"published"`
	Author        *gofeed.Person      `json:"author"`
	Language      string              `json:"language"`
	Favicon       string              `json:"favicon"`
	SiteImage     string              `json:"siteImage,omitempty"`
	Categories    string              `json:"categories"`
	Items         *[]FeedResponseItem `json:"items,omitempty"`
}

type Feeds struct {
	Feeds []FeedResponse `json:"feeds"`
}

type ReaderViewResult struct {
	URL         string `json:"url"`
	Status      string `json:"status"`
	ReaderView  string `json:"content"`
	Error       error  `json:"error,omitempty"`
	Title       string `json:"title"`
	SiteName    string `json:"siteName"`
	Image       string `json:"image"`
	Favicon     string `json:"favicon"`
	TextContent string `json:"textContent"`
}

type FeedSearchAPIResponseItem struct {
	Bozo           int      `json:"bozo"`
	Content_length int      `json:"content_length"`
	Content_type   string   `json:"content_type"`
	Description    string   `json:"description"`
	Favicon        string   `json:"favicon"`
	Hubs           []string `json:"hubs"`
	Is_Podcast     bool     `json:"is_podcast"`
	Is_Push        bool     `json:"is_push"`
	Item_Count     int      `json:"item_count"`
	Last_Seen      string   `json:"last_seen"`
	Last_Updated   string   `json:"last_updated"`
	Score          float64  `json:"score"`
	Site_name      string   `json:"site_name"`
	Site_url       string   `json:"site_url"`
	Title          string   `json:"title"`
	Url            string   `json:"url"`
	Velocity       float64  `json:"velocity"`
	Version        string   `json:"version"`
	Self_Url       string   `json:"self_url"`
}

type FeedSearchResponseItem struct {
	Title        string  `json:"title"`
	Url          string  `json:"url"`
	Site_name    string  `json:"site_name"`
	Site_url     string  `json:"site_url"`
	Description  string  `json:"description"`
	Favicon      string  `json:"favicon"`
	Is_Podcast   bool    `json:"is_podcast"`
	Is_Push      bool    `json:"is_push"`
	Item_Count   int     `json:"item_count"`
	Last_Seen    string  `json:"last_seen"`
	Last_Updated string  `json:"last_updated"`
	Score        float64 `json:"score"`
}

type PodcastSearchResponseItem struct {
	Title             string   `json:"title"`
	Url               string   `json:"url"`
	Author            string   `json:"author"`
	Description       string   `json:"description"`
	FeedImage         string   `json:"feedImage"`
	Image             string   `json:"image"`
	Artwork           string   `json:"artwork"`
	Categories        []string `json:"categories"`
	PodcastGUID       string   `json:"podcastGuid"`
	EpisodeCount      int      `json:"episodeCount"`
	NewestItemPubdate float32  `json:"newestItemPubdate"`
}

type PodcastAPIResponseItem struct {
	Id                     int                         `json:"id"`
	Title                  string                      `json:"title"`
	Url                    string                      `json:"url"`
	OriginalUrl            string                      `json:"originalUrl"`
	Link                   string                      `json:"link"`
	Description            string                      `json:"description"`
	Author                 string                      `json:"author"`
	Language               string                      `json:"language"`
	OwnerName              string                      `json:"ownerName"`
	Image                  string                      `json:"image"`
	Artwork                string                      `json:"artwork"`
	FeedImage              string                      `json:"feedImage"`
	FeedID                 string                      `json:"feedId"`
	PodcastGUID            string                      `json:"podcastGuid"`
	LastUpdatedTime        string                      `json:"lastUpdatedTime"`
	LastCrawlTime          int                         `json:"lastCrawlTime"`
	LastParseTime          int                         `json:"lastParseTime"`
	InPollingQueue         int                         `json:"inPollingQueue"`
	Priority               int                         `json:"priority"`
	LastGoodHttpStatusTime int                         `json:"lastGoodHttpStatusTime"`
	LastHttpStatus         int                         `json:"lastHttpStatus"`
	ContentType            string                      `json:"contentType"`
	ItunedId               int                         `json:"itunedId"`
	Generator              string                      `json:"generator"`
	Dead                   int                         `json:"dead"`
	CrawlErrors            int                         `json:"crawlErrors"`
	ParseErrors            int                         `json:"parseErrors"`
	Categories             []string                    `json:"podCast_Categories"`
	Locked                 int                         `json:"locked"`
	Medium                 string                      `json:"medium"`
	EpisodeCount           int                         `json:"episodeCount"`
	ImageUrlHash           float64                     `json:"imageUrlHash"`
	NewestItemPubdate      float32                     `json:"newestItemPubdate"`
	Transcripts            []PodcastTranscriptsDetails `json:"transcripts"`
}

type PodcastTranscriptsDetails struct {
	Url  string                 `json:"url"`
	Type PodcastTranscriptsType `json:"type"`
}

type PodcastTranscriptsType string

// Define constants of the custom type
const (
	TextHTML       PodcastTranscriptsType = "text/html"
	ApplicationSRT PodcastTranscriptsType = "application/srt"
	ApplicationVTT PodcastTranscriptsType = "application/vtt"
)

type PodcastSearchAPIResponse struct {
	Items       []PodcastAPIResponseItem `json:"feeds"`
	Status      string                   `json:"status"`
	Count       int                      `json:"count"`
	Query       string                   `json:"query"`
	Description string                   `json:"description"`
}

type TTSRequest struct {
	Text         string `json:"text"`
	LanguageCode string `json:"languageCode"`
	SsmlGender   string `json:"ssmlGender"`
	VoiceName    string `json:"voiceName"`
	Url          string `json:"url"`
}

// MetaData Items
type MetaDataResponseItem struct {
	Title       string      `json:"title"`
	Description string      `json:"description"`
	Images      []WebMedia  `json:"images"`
	Type        string      `json:"type"`
	Sitename    string      `json:"sitename"`
	Favicon     string      `json:"favicon"`
	Duration    int         `json:"duration"`
	Domain      string      `json:"domain"`
	URL         string      `json:"url"`
	Videos      []WebMedia  `json:"videos"`
	Locale      string      `json:"locale,omitempty"`
	Determiner  string      `json:"determiner,omitempty"`
	Raw         interface{} `json:"raw,omitempty"`
	ThemeColor  string      `json:"themeColor,omitempty"`
}

// WebMedia captures info about images/videos, including optional metadata.
type WebMedia struct {
	URL         string   `json:"url"`
	Alt         string   `json:"alt,omitempty"`
	Type        string   `json:"type,omitempty"`
	Width       int      `json:"width,omitempty"`
	Height      int      `json:"height,omitempty"`
	Tags        []string `json:"tags,omitempty"`
	SecureURL   string   `json:"secure_url,omitempty"`
	Duration    int      `json:"duration,omitempty"`
	ReleaseDate string   `json:"release_date,omitempty"`
}

// Urls represents a list of URLs in a JSON payload.
type Urls struct {
	Urls []string `json:"urls"`
}

// FeedResult represents the result of discovering an RSS feed for a URL.
type FeedResult struct {
	URL      string `json:"url"`
	Status   string `json:"status"`
	Error    string `json:"error,omitempty"`
	FeedLink string `json:"feedLink"`
}

// createShareRequest represents the expected JSON body for the /create endpoint.
type createShareRequest struct {
	Urls []string `json:"urls"`
}

// fetchShareRequest represents the expected JSON body for the /share endpoint.
type fetchShareRequest struct {
	Key string `json:"key"`
}

// URLValidationRequest represents the request format for URL validation
type URLValidationRequest struct {
	URLs []string `json:"urls"`
}

// URLStatus represents the status of a single URL validation
type URLStatus struct {
	URL    string `json:"url"`
	Status string `json:"status"`
}

// CONSTANTS

const redis_password = ""
const redis_db = 0
const feed_prefix = "feed:"
const metaData_prefix = "metaData:"
const readerView_prefix = "readerViewContent:"
const feedsearch_prefix = "feedsearch:"
const thumbnailColor_prefix = "thumbnailColor:"

const audio_prefix = "tts:"

const DefaultRed = uint8(128)
const DefaultGreen = uint8(128)
const DefaultBlue = uint8(128)

//discover.go
// Package main provides the main functionality for the web server.
package main

import (
	"encoding/json"
	"errors"
	"net/http"
	"net/url"
	"strings"
	"sync"

	"github.com/PuerkitoBio/goquery"
	"github.com/sirupsen/logrus"
)

var (
	feedResultPool = &sync.Pool{
		New: func() interface{} {
			return &FeedResult{}
		},
	}
)

/**
 * @function discoverHandler
 * @description Handles HTTP requests to the /discover endpoint.
 *              It expects a POST request with a JSON body containing an array of URLs.
 *              It attempts to discover the RSS feed URL for each provided URL and returns the results as JSON.
 * @param {http.ResponseWriter} w The HTTP response writer.
 * @param {*http.Request} r The HTTP request.
 * @returns {void}
 * @dependencies discoverRssFeedUrl, generateGitHubRssUrl, generateRedditRssUrl, ensureAbsoluteUrl, httpClient, log
 */
func discoverHandler(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		log.WithFields(logrus.Fields{
			"method": r.Method,
		}).Warn("[discoverHandler] Invalid method")
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
		return
	}
	var urls Urls
	err := json.NewDecoder(r.Body).Decode(&urls)
	if err != nil {
		log.WithFields(logrus.Fields{
			"error": err,
		}).Error("[discoverHandler] Error decoding request body")
		http.Error(w, "Bad Request", http.StatusBadRequest)
		return
	}

	var wg sync.WaitGroup
	results := make([]FeedResult, len(urls.Urls))
	for i, url := range urls.Urls {
		wg.Add(1)
		go func(i int, url string) {
			defer wg.Done()
			feedLink, err := discoverRssFeedUrl(url)
			result := feedResultPool.Get().(*FeedResult)
			defer feedResultPool.Put(result)
			if err != nil {
				result.URL = url
				result.Status = "error"
				result.Error = err.Error()
				result.FeedLink = ""
			} else {
				result.URL = url
				result.Status = "ok"
				result.FeedLink = feedLink
			}
			results[i] = *result
		}(i, url)
	}
	wg.Wait()

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string][]FeedResult{"feeds": results})
}

/**
 * @function discoverRssFeedUrl
 * @description Attempts to discover the RSS feed URL for a given URL.
 *              It first checks if the URL is a GitHub or Reddit URL and generates the feed URL accordingly.
 *              Otherwise, it sends an HTTP GET request to the URL, parses the HTML response,
 *              and searches for link elements with type="application/rss+xml".
 * @param {string} urlStr The URL to discover the RSS feed for.
 * @returns {string, error} The RSS feed URL if found, or an error if not found or an error occurred.
 * @dependencies generateGitHubRssUrl, generateRedditRssUrl, ensureAbsoluteUrl, httpClient, goquery.NewDocumentFromReader, log
 */
func discoverRssFeedUrl(urlStr string) (string, error) {
	if strings.HasPrefix(urlStr, "https://github.com") {
		return generateGitHubRssUrl(urlStr), nil
	}

	if strings.HasPrefix(urlStr, "https://www.reddit.com") {
		return generateRedditRssUrl(urlStr), nil
	}

	req, err := http.NewRequest(http.MethodGet, urlStr, nil)
	if err != nil {
		log.WithFields(logrus.Fields{
			"url":   urlStr,
			"error": err,
		}).Error("[discoverRssFeedUrl] Error creating request")
		return "", err
	}

	// Set a custom User-Agent to avoid being blocked by some websites
	req.Header.Set("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3")

	resp, err := httpClient.Do(req)
	if err != nil {
		log.WithFields(logrus.Fields{
			"url":   urlStr,
			"error": err,
		}).Error("[discoverRssFeedUrl] Error sending request")
		return "", err
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		log.WithFields(logrus.Fields{
			"url":        urlStr,
			"statusCode": resp.StatusCode,
		}).Error("[discoverRssFeedUrl] Request failed")
		return "", errors.New("request failed")
	}

	doc, err := goquery.NewDocumentFromReader(resp.Body)
	if err != nil {
		log.WithFields(logrus.Fields{
			"url":   urlStr,
			"error": err,
		}).Error("[discoverRssFeedUrl] Error parsing HTML")
		return "", err
	}

	rssLink, exists := doc.Find(`link[type="application/rss+xml"]`).Attr("href")
	if !exists {
		log.WithFields(logrus.Fields{
			"url": urlStr,
		}).Warn("[discoverRssFeedUrl] RSS feed not found in HTML")
		return "", errors.New("RSS feed not found")
	}

	rssLink, err = ensureAbsoluteUrl(urlStr, rssLink)
	if err != nil {
		log.WithFields(logrus.Fields{
			"url":     urlStr,
			"rssLink": rssLink,
			"error":   err,
		}).Error("[discoverRssFeedUrl] Error ensuring absolute URL")
		return "", err
	}

	return rssLink, nil
}

/**
 * @function generateGitHubRssUrl
 * @description Generates a GitHub RSS feed URL from a GitHub repository URL.
 * @param {string} url The GitHub repository URL.
 * @returns {string} The corresponding GitHub RSS feed URL.
 */
func generateGitHubRssUrl(url string) string {
	return strings.TrimRight(url, "/") + "/commits/master.atom"
}

/**
 * @function generateRedditRssUrl
 * @description Generates a Reddit RSS feed URL from a Reddit URL.
 * @param {string} url The Reddit URL.
 * @returns {string} The corresponding Reddit RSS feed URL.
 */
func generateRedditRssUrl(url string) string {
	return strings.TrimRight(url, "/") + "/.rss"
}

/**
 * @function ensureAbsoluteUrl
 * @description Ensures that a given URL is absolute. If the URL is relative, it resolves it against the base URL.
 * @param {string} baseUrl The base URL to use for resolving relative URLs.
 * @param {string} relativeOrAbsoluteUrl The URL to ensure is absolute.
 * @returns {string, error} The absolute URL, or an error if the URL is invalid.
 * @dependencies url.Parse, url.Parse
 */
func ensureAbsoluteUrl(baseUrl, relativeOrAbsoluteUrl string) (string, error) {
	u, err := url.Parse(relativeOrAbsoluteUrl)
	if err != nil || !u.IsAbs() {
		u, err = url.Parse(baseUrl)
		if err != nil {
			return "", err
		}
		rel, err := url.Parse(relativeOrAbsoluteUrl)
		if err != nil {
			return "", err
		}
		return u.ResolveReference(rel).String(), nil
	}
	return relativeOrAbsoluteUrl, nil
}

//search.go
// Package main provides the main functionality for the web server.
package main

import (
	"crypto/sha1"
	"encoding/hex"
	"encoding/json"
	"io"
	"net/http"
	"net/url"
	"os"
	"strconv"
	"time"

	"github.com/sirupsen/logrus"
)

/**
 * @function createRSSSearchResponse
 * @description Converts an array of FeedSearchAPIResponseItem to an array of FeedSearchResponseItem.
 * @param {[]FeedSearchAPIResponseItem} apiResults The array of FeedSearchAPIResponseItem to convert.
 * @returns {[]FeedSearchResponseItem} The converted array of FeedSearchResponseItem.
 */
func createRSSSearchResponse(apiResults []FeedSearchAPIResponseItem) []FeedSearchResponseItem {
	var responseItems []FeedSearchResponseItem
	for _, item := range apiResults {
		responseItem := FeedSearchResponseItem{
			Title:        item.Title,
			Url:          item.Url,
			Site_name:    item.Site_name,
			Site_url:     item.Site_url,
			Description:  item.Description,
			Favicon:      item.Favicon,
			Is_Podcast:   item.Is_Podcast,
			Is_Push:      item.Is_Push,
			Item_Count:   item.Item_Count,
			Last_Seen:    item.Last_Seen,
			Last_Updated: item.Last_Updated,
			Score:        item.Score,
		}
		responseItems = append(responseItems, responseItem)
	}
	return responseItems
}

/**
 * @function createPodcastSearchResponse
 * @description Converts an array of PodcastAPIResponseItem to an array of PodcastSearchResponseItem.
 * @param {[]PodcastAPIResponseItem} apiResults The array of PodcastAPIResponseItem to convert.
 * @returns {[]PodcastSearchResponseItem} The converted array of PodcastSearchResponseItem.
 */
func createPodcastSearchResponse(apiResults []PodcastAPIResponseItem) []PodcastSearchResponseItem {
	var responseItems []PodcastSearchResponseItem
	for _, item := range apiResults {
		responseItem := PodcastSearchResponseItem{
			Title:             item.Title,
			Url:               item.Url,
			Author:            item.Author,
			Description:       item.Description,
			FeedImage:         item.Image,
			Image:             item.Image,
			Artwork:           item.Artwork,
			Categories:        item.Categories,
			PodcastGUID:       item.PodcastGUID,
			EpisodeCount:      item.EpisodeCount,
			NewestItemPubdate: item.NewestItemPubdate,
		}
		responseItems = append(responseItems, responseItem)
	}
	return responseItems
}

/**
 * @function searchRSS
 * @description Searches for RSS feeds matching a given URL using an external API.
 *              It caches the results for 24 hours.
 * @param {string} queryURL The URL to search for.
 * @returns {[]FeedSearchResponseItem} An array of FeedSearchResponseItem representing the search results.
 * @dependencies createHash, cache, httpClient, log
 */
func searchRSS(queryURL string) []FeedSearchResponseItem {

	log.WithFields(logrus.Fields{
		"queryURL": queryURL,
	}).Info("[searchRSS] Search request received")
	queryURLCacheKey := createHash(queryURL)

	// Check the cache if the URL has been searched before
	var cachedResults []FeedSearchResponseItem
	if err := cache.Get(feedsearch_prefix, queryURLCacheKey, &cachedResults); err == nil {
		log.WithFields(logrus.Fields{
			"queryURL": queryURL,
		}).Info("[searchRSS] Cache hit")
		return cachedResults
	} else {
		log.WithFields(logrus.Fields{
			"queryURL": queryURL,
		}).Info("[searchRSS] Cache miss")
	}

	// Construct the external API URL
	apiURL := "https://feedsearch.dev/api/v1/search?url=" + url.QueryEscape(queryURL)

	// Make the request to the external API
	resp, err := httpClient.Get(apiURL)
	if err != nil {
		log.WithFields(logrus.Fields{
			"queryURL": queryURL,
			"apiURL":   apiURL,
			"error":    err,
		}).Error("[searchRSS] Error making request to external API")
		return nil
	}
	defer resp.Body.Close()

	// Read the response from the external API
	body, err := io.ReadAll(resp.Body)
	if err != nil {
		log.WithFields(logrus.Fields{
			"queryURL": queryURL,
			"apiURL":   apiURL,
			"error":    err,
		}).Error("[searchRSS] Error reading response from external API")
		return nil
	}

	// Unmarshal the JSON response into the FeedSearchAPIResponseItem structure
	var searchResults []FeedSearchAPIResponseItem
	err = json.Unmarshal(body, &searchResults)
	if err != nil {
		log.WithFields(logrus.Fields{
			"queryURL": queryURL,
			"apiURL":   apiURL,
			"error":    err,
		}).Error("[searchRSS] Error unmarshalling response from external API")
		return nil
	}

	// Convert the API response to the desired response format
	responseItems := createRSSSearchResponse(searchResults)

	// Cache the search results
	if err := cache.Set(feedsearch_prefix, queryURLCacheKey, responseItems, 24*time.Hour); err != nil {
		log.WithFields(logrus.Fields{
			"queryURL": queryURL,
			"error":    err,
		}).Error("[searchRSS] Failed to cache search results")
	} else {
		log.WithFields(logrus.Fields{
			"queryURL": queryURL,
		}).Info("[searchRSS] Successfully cached search results")
	}

	return responseItems
}

/**
 * @function calculateAuth
 * @description Calculates the authorization header for the Podcast Index API.
 * @param {string} key The API key.
 * @param {string} secret The API secret.
 * @param {string} datestr The current date string in Unix timestamp format.
 * @returns {string} The calculated authorization header.
 * @dependencies sha1.New, hex.EncodeToString
 */
func calculateAuth(key, secret, datestr string) string {
	h := sha1.New()
	h.Write([]byte(key + secret + datestr))
	return hex.EncodeToString(h.Sum(nil))
}

/**
 * @function searchPodcast
 * @description Searches for podcasts matching a given query using the Podcast Index API.
 * @param {*http.Request} _ The HTTP request (not used in this function).
 * @param {string} query The search query.
 * @returns {[]PodcastSearchResponseItem} An array of PodcastSearchResponseItem representing the search results.
 * @dependencies calculateAuth, httpClient, log
 */
func searchPodcast(_ *http.Request, query string) []PodcastSearchResponseItem {
	log.WithFields(logrus.Fields{
		"query": query,
	}).Info("[searchPodcast] Search request received")
	key := os.Getenv("PODCAST_INDEX_API_KEY")
	secret := os.Getenv("PODCAST_INDEX_API_SECRET")
	baseURL := "https://api.podcastindex.org/api/1.0/"
	apiURL := baseURL + "search/byterm?q=" + url.QueryEscape(query)

	log.WithFields(logrus.Fields{
		"apiURL": apiURL,
	}).Debug("[searchPodcast] API URL")

	client := &http.Client{}
	req, err := http.NewRequest("GET", apiURL, nil)
	if err != nil {
		log.WithFields(logrus.Fields{
			"query":  query,
			"apiURL": apiURL,
			"error":  err,
		}).Error("[searchPodcast] Error creating request")
		return nil
	}
	now := strconv.FormatInt(time.Now().Unix(), 10)
	authorization := calculateAuth(key, secret, now)

	req.Header.Set("User-Agent", "MyPodcastApp")
	req.Header.Set("X-Auth-Key", key)
	req.Header.Set("X-Auth-Date", now)
	req.Header.Set("Authorization", authorization)
	req.Header.Set("Accept", "application/json")

	resp, err := client.Do(req)
	if err != nil {
		log.WithFields(logrus.Fields{
			"query":  query,
			"apiURL": apiURL,
			"error":  err,
		}).Error("[searchPodcast] Error making request to Podcast Index API")
		return nil
	}
	defer resp.Body.Close()

	body, err := io.ReadAll(resp.Body)
	if err != nil {
		log.WithFields(logrus.Fields{
			"query":  query,
			"apiURL": apiURL,
			"error":  err,
		}).Error("[searchPodcast] Error reading response from Podcast Index API")
		return nil
	}

	var searchResults PodcastSearchAPIResponse
	err = json.Unmarshal(body, &searchResults)
	if err != nil {
		log.WithFields(logrus.Fields{
			"query":       query,
			"apiURL":      apiURL,
			"error":       err,
			"apiResponse": string(body),
		}).Error("[searchPodcast] Error unmarshalling response from Podcast Index API")
		return nil
	}

	responseItems := createPodcastSearchResponse(searchResults.Items)
	return responseItems
}

/**
 * @function searchHandler
 * @description Handles HTTP requests to the /search endpoint.
 *              It supports searching for both RSS feeds and podcasts based on the 'type' query parameter.
 *              It calls the appropriate search function (searchRSS or searchPodcast) and returns the results as JSON.
 * @param {http.ResponseWriter} w The HTTP response writer.
 * @param {*http.Request} r The HTTP request.
 * @returns {void}
 * @dependencies searchRSS, searchPodcast, log
 */
func searchHandler(w http.ResponseWriter, r *http.Request) {
	// Get the 'url' query parameter
	searchType := r.URL.Query().Get("type")
	switch searchType {
	case "rss":
		queryURL := r.URL.Query().Get("q")
		if queryURL == "" {
			log.Warn("[searchHandler] No url provided for RSS search")
			http.Error(w, "No url provided", http.StatusBadRequest)
			response := map[string]string{
				"status": "error",
				"error":  "No url provided",
			}
			w.Header().Set("Content-Type", "application/json")
			json.NewEncoder(w).Encode(response)
			return
		}
		var searchResults []FeedSearchResponseItem = searchRSS(queryURL)
		w.Header().Set("Content-Type", "application/json")
		json.NewEncoder(w).Encode(searchResults)

	case "podcast":
		query := r.URL.Query().Get("q")
		if query == "" {
			log.Warn("[searchHandler] No query provided for podcast search")
			http.Error(w, "No query provided", http.StatusBadRequest)
			response := map[string]string{
				"status": "error",
				"error":  "No query provided",
			}
			w.Header().Set("Content-Type", "application/json")
			json.NewEncoder(w).Encode(response)
			return
		}
		var searchResults []PodcastSearchResponseItem = searchPodcast(r, query)
		w.Header().Set("Content-Type", "application/json")
		json.NewEncoder(w).Encode(searchResults)

	default:
		log.WithFields(logrus.Fields{
			"type": searchType,
		}).Warn("[searchHandler] No or invalid type provided")
		http.Error(w, "No or invalid type provided", http.StatusBadRequest)
		response := map[string]string{
			"status": "error",
			"error":  "No or invalid type provided",
		}
		w.Header().Set("Content-Type", "application/json")
		json.NewEncoder(w).Encode(response)
	}
}

//share.go
// Package main provides the main functionality for the web server.
package main

import (
	"encoding/json"
	"net/http"
	"time"

	"math/rand"

	"github.com/sirupsen/logrus"
)

/**
 * @function createShareHandler
 * @description Handles HTTP requests to the /create endpoint.
 *              It expects a POST request with a JSON body containing an array of URLs.
 *              It generates a random 6-character key, stores the URLs in the cache with that key,
 *              and returns a link to the /share endpoint with the generated key.
 * @param {http.ResponseWriter} w The HTTP response writer.
 * @param {*http.Request} r The HTTP request.
 * @returns {void}
 * @dependencies cache, log, rand.Seed, rand.Intn
 */
func createShareHandler(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		log.WithFields(logrus.Fields{
			"method": r.Method,
		}).Warn("[createShareHandler] Invalid method")
		http.Error(w, "Invalid method", http.StatusMethodNotAllowed)
		return
	}
	var req createShareRequest
	err := json.NewDecoder(r.Body).Decode(&req)
	if err != nil {
		log.WithFields(logrus.Fields{
			"error": err,
		}).Error("[createShareHandler] Error decoding request body")
		http.Error(w, "Invalid request body", http.StatusBadRequest)
		return
	}

	if len(req.Urls) == 0 {
		log.Warn("[createShareHandler] No URLs provided")
		http.Error(w, "No URLs provided", http.StatusBadRequest)
		response := map[string]string{
			"status": "error",
			"error":  "No URLs provided",
		}
		w.Header().Set("Content-Type", "application/json")
		json.NewEncoder(w).Encode(response)
		return
	}

	log.WithFields(logrus.Fields{
		"urls": req.Urls,
	}).Info("[createShareHandler] Create request received")

	// Generate a random key of maximum 6 characters
	rand.Seed(time.Now().UnixNano())
	chars := []rune("abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ")
	randomKey := make([]rune, 6)
	for i := range randomKey {
		randomKey[i] = chars[rand.Intn(len(chars))]
	}

	cacheKey := "share:" + string(randomKey)

	// Save the URLs to the cache
	err = cache.Set(cacheKey, "urls", req.Urls, 0) // setting exp 0 to keep it forever
	if err != nil {
		log.WithFields(logrus.Fields{
			"key":   cacheKey,
			"urls":  req.Urls,
			"error": err,
		}).Error("[createShareHandler] Failed to save shared URLs")
		http.Error(w, "Failed to save shared URLs", http.StatusInternalServerError)
		return
	}

	// Respond with the link
	response := map[string]string{"status": "ok", "link": "https://www.digests.app/share/" + string(randomKey)}
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(response)
}

/**
 * @function shareHandler
 * @description Handles HTTP requests to the /share endpoint.
 *              It expects a POST request with a JSON body containing a key.
 *              It retrieves the URLs associated with that key from the cache and returns them as JSON.
 * @param {http.ResponseWriter} w The HTTP response writer.
 * @param {*http.Request} r The HTTP request.
 * @returns {void}
 * @dependencies cache, log
 */
func shareHandler(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		log.WithFields(logrus.Fields{
			"method": r.Method,
		}).Warn("[shareHandler] Invalid method")
		http.Error(w, "Invalid method", http.StatusMethodNotAllowed)
		return
	}
	log.Info("[shareHandler] Share request received")

	// Decode the request body
	var req fetchShareRequest
	err := json.NewDecoder(r.Body).Decode(&req)
	if err != nil {
		log.WithFields(logrus.Fields{
			"error": err,
		}).Error("[shareHandler] Error decoding request body")
		http.Error(w, "Invalid request body", http.StatusBadRequest)
		return
	}

	// Get the key from the request body
	cacheKey := "share:" + req.Key

	// Get the URLs from the cache
	var urls []string
	err = cache.Get(cacheKey, "urls", &urls)
	if err != nil {
		log.WithFields(logrus.Fields{
			"key":   cacheKey,
			"error": err,
		}).Error("[shareHandler] Error getting URLs from cache")
		http.Error(w, "Invalid share link", http.StatusBadRequest)
		return
	}

	// Respond with the URLs
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(urls)

	log.WithFields(logrus.Fields{
		"key":  req.Key,
		"urls": urls,
	}).Info("[shareHandler] Share request processed")
}

//utils.go
// Package main provides the main functionality for the web server.
package main

import (
	"encoding/json"
	"net/http"
	"sync"

	"github.com/sirupsen/logrus"
)

// validateURLsHandler handles the /validate endpoint
func validateURLsHandler(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		log.WithFields(logrus.Fields{
			"method": r.Method,
		}).Warn("[validateURLsHandler] Invalid method")
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
		return
	}

	var req URLValidationRequest
	err := json.NewDecoder(r.Body).Decode(&req)
	if err != nil {
		log.WithFields(logrus.Fields{
			"error": err,
		}).Error("[validateURLsHandler] Error decoding request body")
		http.Error(w, err.Error(), http.StatusBadRequest)
		return
	}

	var wg sync.WaitGroup
	statuses := make([]URLStatus, len(req.URLs))
	for i, url := range req.URLs {
		wg.Add(1)
		go func(i int, url string) {
			defer wg.Done()
			resp, err := httpClient.Get(url)
			if resp != nil {
				defer resp.Body.Close()
			}
			status := "ok"
			if err != nil || resp.StatusCode != http.StatusOK {
				status = "error"
				if err == nil {
					status = http.StatusText(resp.StatusCode)
				}
				log.WithFields(logrus.Fields{
					"url":    url,
					"status": status,
					"error":  err,
				}).Warn("[validateURLsHandler] URL validation failed")
			} else {
				log.WithFields(logrus.Fields{
					"url":    url,
					"status": status,
				}).Debug("[validateURLsHandler] URL validation successful")
			}
			statuses[i] = URLStatus{URL: url, Status: status}
		}(i, url)
	}

	wg.Wait()

	w.Header().Set("Content-Type", "application/json")
	err = json.NewEncoder(w).Encode(statuses)
	if err != nil {
		log.WithFields(logrus.Fields{
			"error": err,
		}).Error("[validateURLsHandler] Error encoding response")
		http.Error(w, err.Error(), http.StatusInternalServerError)
	}
}

//ImageUtils.go
// Package main provides the main functionality for the web server.
package main

import (
	"context"
	"encoding/json"
	"fmt"
	"image"
	"image/draw"
	"net/http"
	"net/url"
	"strconv"
	"strings"
	"sync"
	"time"

	"golang.org/x/net/html"

	"github.com/EdlinOrg/prominentcolor"
	"github.com/PuerkitoBio/goquery"
	"github.com/gocolly/colly"
	"github.com/mmcdole/gofeed"
	"github.com/sirupsen/logrus"
)

const (
	thumbnailColorPrefix = thumbnailColor_prefix
	httpTimeout          = 10 * time.Second
	cacheDuration        = 24 * time.Hour
	defaultColor         = 128
	userAgent            = "Mozilla/5.0 (compatible; FeedParser/1.0)"
	collyUserAgent       = "facebookexternalhit/1.1 (+http://www.facebook.com/externalhit_uatext.php)"
)

type ThumbnailFinder struct {
	cache sync.Map
}

func NewThumbnailFinder() *ThumbnailFinder {
	return &ThumbnailFinder{}
}

/**
 * @function GetMetaData
 * @description Fetches a web page (targetURL) using Colly, extracting Open Graph tags
 * and JSON-LD data to produce a MetaDataResponseItem. It also attempts to discover
 * the favicon and domain if not provided by OG or JSON-LD.
 * @param {string} targetURL The URL to visit and parse.
 * @returns {MetaDataResponseItem, error} On success, returns a struct with combined
 * metadata from both OG tags and JSON-LD. Returns an error if the fetch or parse fails.
 * @dependencies colly.NewCollector, colly.UserAgent, c.OnHTML, c.OnRequest, c.Visit, url.Parse, strconv.Atoi, json.Unmarshal, goquery.Selection, log
 */
func GetMetaData(targetURL string) (MetaDataResponseItem, error) {
	// Basic validation
	if targetURL == "" || targetURL == "http://" || targetURL == "://" || targetURL == "about:blank" {
		log.WithFields(logrus.Fields{
			"url": targetURL,
		}).Error("[GetMetaData] URL is empty or invalid")
		return MetaDataResponseItem{}, fmt.Errorf("URL is empty or invalid")
	}

	// Prepare the collector with a friendly user agent that tricks sites
	c := colly.NewCollector(
		colly.UserAgent(collyUserAgent),
	)

	// Our enriched metadata struct
	metaData := MetaDataResponseItem{
		Images: []WebMedia{},
		Videos: []WebMedia{},
	}

	// STEP 1: OnHTML("meta", ...) to capture Open Graph fields.
	c.OnHTML("meta", func(e *colly.HTMLElement) {
		property := e.Attr("property")
		content := e.Attr("content")
		name := e.Attr("name")
		if (property == "" && name == "") || content == "" {
			return
		}

		// Check for og: fields
		parts := strings.Split(property, ":")
		if len(parts) < 2 || parts[0] != "og" {
			// Check for theme-color
			if name == "theme-color" {
				metaData.ThemeColor = content
			}
			return
		}

		switch property {
		case "og:title":
			metaData.Title = content
		case "og:description":
			metaData.Description = content
		case "og:site_name":
			metaData.Sitename = content
		case "og:url":
			metaData.URL = content
		case "og:type":
			metaData.Type = content
		case "og:locale":
			metaData.Locale = content
		case "og:determiner":
			metaData.Determiner = content

		// Images
		case "og:image":
			metaData.Images = append(metaData.Images, WebMedia{URL: content})
		case "og:image:width", "og:image:height", "og:image:alt", "og:image:type", "og:image:secure_url":
			if len(metaData.Images) > 0 {
				idx := len(metaData.Images) - 1
				switch property {
				case "og:image:width":
					if w, err := strconv.Atoi(content); err == nil {
						metaData.Images[idx].Width = w
					}
				case "og:image:height":
					if h, err := strconv.Atoi(content); err == nil {
						metaData.Images[idx].Height = h
					}
				case "og:image:alt":
					metaData.Images[idx].Alt = content
				case "og:image:type":
					metaData.Images[idx].Type = content
				case "og:image:secure_url":
					metaData.Images[idx].SecureURL = content
				}
			}

		// Videos
		case "og:video:url":
			metaData.Videos = append(metaData.Videos, WebMedia{URL: content})
		case "og:video:width", "og:video:height", "og:video:type", "og:video:secure_url", "og:video:duration", "og:video:release_date", "og:video:tag":
			if len(metaData.Videos) > 0 {
				idx := len(metaData.Videos) - 1
				switch property {
				case "og:video:width":
					if w, err := strconv.Atoi(content); err == nil {
						metaData.Videos[idx].Width = w
					}
				case "og:video:height":
					if h, err := strconv.Atoi(content); err == nil {
						metaData.Videos[idx].Height = h
					}
				case "og:video:type":
					metaData.Videos[idx].Type = content
				case "og:video:secure_url":
					metaData.Videos[idx].SecureURL = content
				case "og:video:duration":
					if d, err := strconv.Atoi(content); err == nil {
						metaData.Videos[idx].Duration = d
					}
				case "og:video:release_date":
					metaData.Videos[idx].ReleaseDate = content
				case "og:video:tag":
					tags := strings.Split(content, ",")
					for _, t := range tags {
						metaData.Videos[idx].Tags = append(metaData.Videos[idx].Tags, strings.TrimSpace(t))
					}
				}
			}
		}
	})

	// STEP 2: OnHTML("script[type='application/ld+json']", ...) to parse JSON-LD.
	c.OnHTML("script[type='application/ld+json']", func(e *colly.HTMLElement) {
		rawJSON := e.Text
		// Attempt to parse the JSON-LD
		var ldData interface{}
		if err := json.Unmarshal([]byte(rawJSON), &ldData); err == nil {
			// If we have no JSONLD in metaData, store this entire ldData.
			// If you want to further parse Title, etc., you can do so here.
			metaData.Raw = ldData
			// Optionally, you can write logic to unify some fields from ld+json
			// with your metaData if you want to override or fill in missing values.
		}
	})

	// STEP 3: Attempt domain detection on request
	c.OnRequest(func(r *colly.Request) {
		if metaData.Domain == "" {
			if parsedURL, err := url.Parse(r.URL.String()); err == nil {
				metaData.Domain = parsedURL.Host
			}
		}
	})

	// STEP 4: After we parse <head>, try to find a favicon if none is set
	c.OnHTML("head", func(e *colly.HTMLElement) {
		if metaData.Favicon == "" {
			e.DOM.Find("link[rel]").Each(func(_ int, s *goquery.Selection) {
				rel := s.AttrOr("rel", "")
				href := s.AttrOr("href", "")
				relValues := strings.Fields(rel)
				for _, rv := range relValues {
					if rv == "icon" || rv == "shortcut" || rv == "apple-touch-icon" {
						if href != "" {
							metaData.Favicon = e.Request.AbsoluteURL(href)
							return
						}
					}
				}
			})
		}
	})

	// STEP 5: Visit the target page
	if err := c.Visit(targetURL); err != nil {
		log.WithFields(logrus.Fields{
			"url":   targetURL,
			"error": err,
		}).Error("[GetMetaData] Error visiting URL")
		return MetaDataResponseItem{}, fmt.Errorf("error visiting URL %s: %w", targetURL, err)
	}

	return metaData, nil
}

/**
 * @function FindThumbnailForItem
 * @description Finds a thumbnail for a given feed item.
 *              It first checks the cache, then enclosures, then content, and finally fetches metadata.
 * @param {*gofeed.Item} item The feed item to find a thumbnail for.
 * @returns {string} The URL of the thumbnail, or an empty string if no thumbnail was found.
 * @dependencies extractThumbnailFromEnclosures, extractThumbnailFromContent, GetMetaData, log
 */
func (tf *ThumbnailFinder) FindThumbnailForItem(item *gofeed.Item) string {
	if thumb, ok := tf.cache.Load(item.Link); ok {
		log.WithFields(logrus.Fields{
			"url": item.Link,
		}).Debug("[FindThumbnailForItem] Found cached thumbnail")
		return thumb.(string)
	}

	thumbnail := tf.extractThumbnailFromEnclosures(item.Enclosures)
	if thumbnail != "" {
		log.WithFields(logrus.Fields{
			"url": item.Link,
		}).Debug("[FindThumbnailForItem] Found thumbnail in enclosures")
		tf.cache.Store(item.Link, thumbnail)
		return thumbnail
	}

	thumbnail = tf.extractThumbnailFromContent(item.Content)
	if thumbnail != "" {
		log.WithFields(logrus.Fields{
			"url": item.Link,
		}).Debug("[FindThumbnailForItem] Found thumbnail in content")
		tf.cache.Store(item.Link, thumbnail)
		return thumbnail
	}

	if item.Link != "" {
		log.WithFields(logrus.Fields{
			"url": item.Link,
		}).Debug("[FindThumbnailForItem] Fetching metadata")
		metaData, err := GetMetaData(item.Link)
		if err != nil {
			log.WithFields(logrus.Fields{
				"url":   item.Link,
				"error": err,
			}).Error("[FindThumbnailForItem] Error getting metadata")
			return ""
		}

		if len(metaData.Images) > 0 {
			thumbnail = metaData.Images[0].URL
		}
		if thumbnail != "" {
			log.WithFields(logrus.Fields{
				"url": item.Link,
			}).Debug("[FindThumbnailForItem] Found thumbnail in metadata")
			tf.cache.Store(item.Link, thumbnail)
			return thumbnail
		}

		return ""
	}
	return ""
}

/**
 * @function extractThumbnailFromEnclosures
 * @description Extracts a thumbnail URL from a list of enclosures.
 * @param {[]*gofeed.Enclosure} enclosures The list of enclosures to search.
 * @returns {string} The URL of the first image enclosure found, or an empty string if none were found.
 */
func (tf *ThumbnailFinder) extractThumbnailFromEnclosures(enclosures []*gofeed.Enclosure) string {
	for _, e := range enclosures {
		if strings.HasPrefix(e.Type, "image/") {
			return e.URL
		}
	}
	return ""
}

/**
 * @function extractThumbnailFromContent
 * @description Extracts a thumbnail URL from HTML content.
 * @param {string} content The HTML content to search.
 * @returns {string} The URL of the first image found in the content, or an empty string if none were found.
 * @dependencies goquery.NewDocumentFromReader, log
 */
func (tf *ThumbnailFinder) extractThumbnailFromContent(content string) string {
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(content))
	if err != nil {
		log.WithFields(logrus.Fields{
			"error": err,
		}).Error("[extractThumbnailFromContent] Error parsing content")
		return ""
	}

	if src, exists := doc.Find("img").First().Attr("src"); exists {
		return src
	}
	return ""
}

/**
 * @function extractColorFromThumbnail_prominentColor
 * @description Extracts the prominent color from an image URL using the prominentcolor library.
 *              It first checks the cache, then validates the URL, downloads the image, decodes it,
 *              and finally extracts the color using the K-means algorithm.
 * @param {string} imageURL The URL of the image to extract the color from.
 * @returns {r, g, b uint8} The red, green, and blue components of the prominent color.
 * @dependencies httpClient, cache, prominentcolor, image.Decode, log
 */
func extractColorFromThumbnail_prominentColor(imageURL string) (r, g, b uint8) {
	defer func() {
		if rec := recover(); rec != nil {
			log.WithFields(logrus.Fields{
				"url":   imageURL,
				"panic": rec,
			}).Error("[extractColorFromThumbnail_prominentColor] Recovered from panic")
			r, g, b = 128, 128, 128
		}
	}()

	if imageURL == "" {
		return 128, 128, 128
	}

	cachePrefix := thumbnailColorPrefix
	var cachedColor RGBColor

	// Attempt to retrieve the color from the cache
	err := cache.Get(cachePrefix, imageURL, &cachedColor)
	if err == nil {
		log.WithFields(logrus.Fields{
			"url":   imageURL,
			"color": cachedColor,
		}).Debug("[extractColorFromThumbnail_prominentColor] Found cached color")
		return cachedColor.R, cachedColor.G, cachedColor.B
	}

	// Validate the image URL
	parsedURL, err := url.Parse(imageURL)
	if err != nil || parsedURL.Scheme == "" || parsedURL.Host == "" {
		log.WithFields(logrus.Fields{
			"url":   imageURL,
			"error": err,
		}).Error("[extractColorFromThumbnail_prominentColor] Invalid image URL")
		cacheDefaultColor(imageURL)
		return 128, 128, 128
	}

	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
	defer cancel()

	req, err := http.NewRequestWithContext(ctx, http.MethodGet, imageURL, nil)
	if err != nil {
		log.WithFields(logrus.Fields{
			"url":   imageURL,
			"error": err,
		}).Error("[extractColorFromThumbnail_prominentColor] Error creating request")
		cacheDefaultColor(imageURL)
		return 128, 128, 128
	}

	resp, err := httpClient.Do(req)
	if err != nil {
		log.WithFields(logrus.Fields{
			"url":   imageURL,
			"error": err,
		}).Error("[extractColorFromThumbnail_prominentColor] Failed to download image")
		cacheDefaultColor(imageURL)
		return 128, 128, 128
	}
	defer resp.Body.Close()

	img, _, err := image.Decode(resp.Body)
	if err != nil {
		log.WithFields(logrus.Fields{
			"url":   imageURL,
			"error": err,
		}).Error("[extractColorFromThumbnail_prominentColor] Failed to decode image")
		cacheDefaultColor(imageURL)
		return 128, 128, 128
	}
	log.WithFields(logrus.Fields{
		"url": imageURL,
	}).Debug("[extractColorFromThumbnail_prominentColor] Starting color extraction")

	bounds := img.Bounds()
	imgNRGBA := image.NewNRGBA(bounds)
	draw.Draw(imgNRGBA, bounds, img, bounds.Min, draw.Src)

	if imgNRGBA == nil {
		log.WithFields(logrus.Fields{
			"url": imageURL,
		}).Error("[extractColorFromThumbnail_prominentColor] imgNRGBA is nil")
		cacheDefaultColor(imageURL)
		return 128, 128, 128
	}

	colors, err := prominentcolor.KmeansWithAll(prominentcolor.ArgumentDefault, imgNRGBA, prominentcolor.DefaultK, 1, prominentcolor.GetDefaultMasks())
	if err != nil || len(colors) == 0 {
		log.WithFields(logrus.Fields{
			"url":   imageURL,
			"error": err,
		}).Error("[extractColorFromThumbnail_prominentColor] Error extracting prominent color with background mask")
		colors, err = prominentcolor.KmeansWithAll(prominentcolor.ArgumentDefault, imgNRGBA, prominentcolor.DefaultK, 1, nil)
		if err != nil || len(colors) == 0 {
			log.WithFields(logrus.Fields{
				"url":   imageURL,
				"error": err,
			}).Error("[extractColorFromThumbnail_prominentColor] Error extracting prominent color without background mask")
			cacheDefaultColor(imageURL)
			return 128, 128, 128
		}
	}

	if len(colors) > 0 {
		extractedColor := RGBColor{uint8(colors[0].Color.R), uint8(colors[0].Color.G), uint8(colors[0].Color.B)}
		log.WithFields(logrus.Fields{
			"url":   imageURL,
			"color": extractedColor,
		}).Debug("[extractColorFromThumbnail_prominentColor] Extracted color")
		if err := cache.Set(cachePrefix, imageURL, extractedColor, 24*time.Hour); err != nil {
			log.WithFields(logrus.Fields{
				"url":   imageURL,
				"color": extractedColor,
				"error": err,
			}).Error("[extractColorFromThumbnail_prominentColor] Failed to cache color")
		}
		return extractedColor.R, extractedColor.G, extractedColor.B
	}

	// Cache the default color if extraction fails
	cacheDefaultColor(imageURL)
	return 128, 128, 128
}

/**
 * @function cacheDefaultColor
 * @description Caches the default color for a given image URL.
 * @param {string} imageURL The URL of the image to cache the default color for.
 * @dependencies cache, log
 */
func cacheDefaultColor(imageURL string) {
	cachePrefix := thumbnailColorPrefix
	defaultColor := RGBColor{defaultColor, defaultColor, defaultColor}

	if err := cache.Set(cachePrefix, imageURL, defaultColor, cacheDuration); err != nil {
		log.WithFields(logrus.Fields{
			"url":   imageURL,
			"error": err,
		}).Error("[cacheDefaultColor] Failed to cache default color")
	}
}

/**
 * @function DiscoverFavicon
 * @description Discovers the favicon for a given page URL.
 *              It sends an HTTP GET request to the page, parses the HTML response,
 *              and searches for link elements with rel attributes containing "icon", "shortcut", or "apple-touch-icon".
 * @param {string} pageURL The URL of the page to discover the favicon for.
 * @returns {string} The URL of the favicon, or an empty string if no favicon was found.
 * @dependencies httpClient, html.Parse, findFavicon, url.Parse, log
 */
func DiscoverFavicon(pageURL string) string {
	ctx, cancel := context.WithTimeout(context.Background(), httpTimeout)
	defer cancel()

	req, err := http.NewRequestWithContext(ctx, http.MethodGet, pageURL, nil)
	if err != nil {
		log.WithFields(logrus.Fields{
			"url":   pageURL,
			"error": err,
		}).Error("[DiscoverFavicon] Error creating request")
		return ""
	}

	req.Header.Set("User-Agent", userAgent)

	resp, err := httpClient.Do(req)
	if err != nil {
		log.WithFields(logrus.Fields{
			"url":   pageURL,
			"error": err,
		}).Error("[DiscoverFavicon] Error fetching page")
		return ""
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		log.WithFields(logrus.Fields{
			"url":    pageURL,
			"status": resp.Status,
		}).Error("[DiscoverFavicon] Error fetching page")
		return ""
	}

	doc, err := html.Parse(resp.Body)
	if err != nil {
		log.WithFields(logrus.Fields{
			"url":   pageURL,
			"error": err,
		}).Error("[DiscoverFavicon] Error parsing HTML")
		return ""
	}

	favicon := findFavicon(doc)

	if favicon != "" && !strings.HasPrefix(favicon, "http") {
		if parsedFaviconURL, err := url.Parse(favicon); err == nil {
			if baseURL, err := url.Parse(pageURL); err == nil {
				favicon = baseURL.ResolveReference(parsedFaviconURL).String()
			}
		}
	}

	return favicon
}

/**
 * @function findFavicon
 * @description Recursively searches an HTML node tree for a favicon link.
 * @param {*html.Node} n The root node of the HTML tree to search.
 * @returns {string} The URL of the favicon, or an empty string if no favicon was found.
 */
func findFavicon(n *html.Node) string {
	if n.Type == html.ElementNode && n.Data == "link" {
		relAttr := ""
		hrefAttr := ""
		for _, attr := range n.Attr {
			if attr.Key == "rel" {
				relAttr = attr.Val
			} else if attr.Key == "href" {
				hrefAttr = attr.Val
			}
		}

		relValues := strings.Fields(relAttr)
		for _, relValue := range relValues {
			if relValue == "icon" || relValue == "shortcut" || relValue == "apple-touch-icon" {
				if hrefAttr != "" {
					return hrefAttr
				}
			}
		}
	}

	for c := n.FirstChild; c != nil; c = c.NextSibling {
		if favicon := findFavicon(c); favicon != "" {
			return favicon
		}
	}

	return ""
}

/**
 * @function DiscoverFaviconWithColly
 * @description Discovers the favicon for a given page URL using colly.
 * @param {*colly.Collector} c The colly collector to use for the request.
 * @param {string} pageURL The URL of the page to discover the favicon for.
 * @returns {string} The URL of the favicon, or an empty string if no favicon was found.
 * @dependencies colly.HTMLElement, c.OnHTML, c.Visit
 */
func DiscoverFaviconWithColly(c *colly.Collector, pageURL string) string {
	var favicon string

	c.OnHTML("link[rel]", func(e *colly.HTMLElement) {
		rel := e.Attr("rel")
		href := e.Attr("href")

		relValues := strings.Fields(rel)
		for _, relValue := range relValues {
			if relValue == "icon" || relValue == "shortcut" || relValue == "apple-touch-icon" {
				if href != "" {
					favicon = e.Request.AbsoluteURL(href)
					return
				}
			}
		}
	})

	c.Visit(pageURL)

	return favicon
}

//parser.go
// Package main provides the main functionality for the web server.
package main

import (
	"crypto/sha256"
	"encoding/hex"
	"encoding/json"
	"net"
	"net/http"
	"net/url"
	"sort"
	"strconv"
	"strings"
	"sync"
	"time"

	"golang.org/x/net/html"

	"github.com/mmcdole/gofeed"
	"github.com/sirupsen/logrus"
)

// The date/time layout format used throughout the code.
const layout = "2006-01-02T15:04:05Z07:00"

/**
 * @function createHash
 * @description Returns a SHA-256 hash of the given string s.
 * @param {string} s The string to hash.
 * @returns {string} The SHA-256 hash of the string.
 */
func createHash(s string) string {
	sum := sha256.Sum256([]byte(s))
	return hex.EncodeToString(sum[:])
}

/**
 * @function parseHTMLContent
 * @description Attempts to parse htmlContent as HTML,
 * extracting and returning only the text content. If parsing fails,
 * the original htmlContent is returned unchanged.
 * @param {string} htmlContent The HTML content to parse.
 * @returns {string} The extracted text content, or the original htmlContent if parsing fails.
 * @dependencies html.Parse, log
 */
func parseHTMLContent(htmlContent string) string {
	doc, err := html.Parse(strings.NewReader(htmlContent))
	if err != nil {
		// Fallback to the raw HTML if parse fails
		log.WithFields(logrus.Fields{
			"error": err,
		}).Warn("[parseHTMLContent] Failed to parse HTML content")
		return htmlContent
	}
	var f func(*html.Node)
	var textContent strings.Builder

	f = func(n *html.Node) {
		if n.Type == html.TextNode {
			textContent.WriteString(n.Data)
		}
		for c := n.FirstChild; c != nil; c = c.NextSibling {
			f(c)
		}
	}
	f(doc)
	return textContent.String()
}

/**
 * @function getBaseDomain
 * @description Attempts to parse rawURL and returns its scheme + hostname (e.g., https://example.com).
 *              If parsing fails, an empty string is returned.
 * @param {string} rawURL The URL to parse.
 * @returns {string} The base domain (scheme + hostname) of the URL, or an empty string if parsing fails.
 * @dependencies url.Parse, log
 */
func getBaseDomain(rawURL string) string {
	parsedURL, err := url.Parse(rawURL)
	if err != nil {
		log.WithFields(logrus.Fields{
			"url":   rawURL,
			"error": err,
		}).Warn("[getBaseDomain] Failed to parse URL")
		return ""
	}
	return parsedURL.Scheme + "://" + parsedURL.Host
}

/**
 * @function metadataHandler
 * @description Handles HTTP requests to the /metadata endpoint.
 *              It expects a POST request with a JSON body containing an array of URLs.
 *              It fetches metadata for each URL using GetMetaData and returns the results as JSON.
 * @param {http.ResponseWriter} w The HTTP response writer.
 * @param {*http.Request} r The HTTP request.
 * @returns {void}
 * @dependencies GetMetaData, log
 */
func metadataHandler(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		log.WithFields(logrus.Fields{
			"method": r.Method,
		}).Warn("[metadataHandler] Invalid method")
		http.Error(w, "Only POST method is allowed", http.StatusMethodNotAllowed)
		return
	}

	var urls Urls
	err := json.NewDecoder(r.Body).Decode(&urls)
	if err != nil {
		log.WithFields(logrus.Fields{
			"error": err,
		}).Error("[metadataHandler] Error decoding request body")
		http.Error(w, "Bad Request", http.StatusBadRequest)
		return
	}

	var wg sync.WaitGroup
	results := make([]MetaDataResponseItem, len(urls.Urls))

	for i, url := range urls.Urls {
		wg.Add(1)
		go func(i int, url string) {
			defer wg.Done()
			log.WithFields(logrus.Fields{
				"url": url,
			}).Debug("[metadataHandler] Fetching metadata")
			result, err := GetMetaData(url)
			if err != nil {
				log.WithFields(logrus.Fields{
					"url":   url,
					"error": err,
				}).Error("[metadataHandler] Error fetching metadata")
			} else {
				results[i] = result
			}
		}(i, url)
	}

	wg.Wait()

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string][]MetaDataResponseItem{"metadata": results})
}

/**
 * @function parseHandler
 * @description Handles HTTP requests to the /parse endpoint.
 *              It expects a POST request with a JSON body containing an array of feed URLs to parse,
 *              along with optional pagination parameters (Page, ItemsPerPage).
 *              It processes each feed URL, applies pagination, and returns the results as JSON.
 * @param {http.ResponseWriter} w The HTTP response writer.
 * @param {*http.Request} r The HTTP request.
 * @returns {void}
 * @dependencies decodeRequest, processURLs, sendResponse, log
 */
func parseHandler(w http.ResponseWriter, r *http.Request) {
	if r.Method != http.MethodPost {
		log.WithFields(logrus.Fields{
			"method": r.Method,
		}).Warn("[parseHandler] Invalid method")
		http.Error(w, "Only POST method is allowed", http.StatusMethodNotAllowed)
		return
	}

	// Default values for pagination
	page := 1
	itemsPerPage := 50

	// Decode request into ParseRequest
	req, err := decodeRequest(r)
	if err != nil {
		log.WithFields(logrus.Fields{
			"error": err,
		}).Error("[parseHandler] Error decoding request body")
		http.Error(w, "Invalid request body", http.StatusBadRequest)
		return
	}

	// If user provided a page > 0, use it; otherwise keep default
	if req.Page > 0 {
		page = req.Page
	}
	// If user provided itemsPerPage > 0, use it; otherwise keep default
	if req.ItemsPerPage > 0 {
		itemsPerPage = req.ItemsPerPage
	}

	responses := processURLs(req.URLs, page, itemsPerPage)
	sendResponse(w, responses)
}

/**
 * @function decodeRequest
 * @description Reads and unmarshals the request body into a ParseRequest object.
 * @param {*http.Request} r The HTTP request.
 * @returns {ParseRequest, error} The parsed ParseRequest object, or an error if unmarshalling fails.
 */
func decodeRequest(r *http.Request) (ParseRequest, error) {
	var req ParseRequest
	err := json.NewDecoder(r.Body).Decode(&req)
	return req, err
}

/**
 * @function processURLs
 * @description Concurrently processes each feed URL using processURL.
 *              Returns all feed responses (with pagination applied) as a slice.
 * @param {[]string} urls The list of feed URLs to process.
 * @param {int} page The page number for pagination.
 * @param {int} itemsPerPage The number of items per page for pagination.
 * @returns {[]FeedResponse} A slice of FeedResponse objects, with pagination applied.
 * @dependencies processURL, numWorkers, log
 */
func processURLs(urls []string, page, itemsPerPage int) []FeedResponse {
	var wg sync.WaitGroup
	sem := make(chan struct{}, numWorkers)
	responses := make(chan FeedResponse, len(urls))

	for _, url := range urls {
		wg.Add(1)
		sem <- struct{}{}
		go func(feedURL string) {
			defer func() {
				wg.Done()
				<-sem
			}()
			log.WithFields(logrus.Fields{
				"url": feedURL,
			}).Debug("[processURLs] Processing feed")
			response := processURL(feedURL, page, itemsPerPage)
			responses <- response
		}(url)
	}

	go func() {
		wg.Wait()
		close(responses)
	}()

	return collectResponses(responses)
}

/**
 * @function isCacheStale
 * @description Checks whether lastRefreshed is older than refresh_timer (in minutes).
 * @param {string} lastRefreshed The timestamp of the last refresh, in the format defined by the 'layout' constant.
 * @returns {bool} True if the cache is stale, false otherwise.
 * @dependencies refresh_timer, time.Parse, time.Since, log
 */
func isCacheStale(lastRefreshed string) bool {
	parsedTime, err := time.Parse(layout, lastRefreshed)
	if err != nil {
		log.WithFields(logrus.Fields{
			"lastRefreshed": lastRefreshed,
			"error":         err,
		}).Error("[isCacheStale] Failed to parse LastRefreshed")
		return false
	}

	if time.Since(parsedTime) > time.Duration(refresh_timer)*time.Minute {
		log.WithFields(logrus.Fields{
			"lastRefreshed": lastRefreshed,
			"refresh_timer": refresh_timer,
		}).Info("[isCacheStale] Cache is stale")
		return true
	}
	return false
}

/**
 * @function fetchAndCacheFeed
 * @description Fetches the remote feed from feedURL, merges with existing items (if any),
 *              and caches the final FeedResponse. Returns the FeedResponse or an error if any step fails.
 * @param {string} feedURL The URL of the feed to fetch.
 * @param {string} cacheKey The key to use for caching the feed.
 * @returns {FeedResponse, error} The fetched and processed FeedResponse, or an error if any step fails.
 * @dependencies gofeed.NewParser, processFeedItems, mergeFeedItemsAtParserLevel, getBaseDomain, addURLToList,
 *               cache, GetMetaData, createFeedResponse, log
 */
func fetchAndCacheFeed(feedURL, cacheKey string) (FeedResponse, error) {
	parser := gofeed.NewParser()
	feed, err := parser.ParseURL(feedURL)
	if err != nil {
		log.WithFields(logrus.Fields{
			"url":   feedURL,
			"error": err,
		}).Error("[fetchAndCacheFeed] Failed to parse feedURL")
		return FeedResponse{}, err
	}

	// Convert newly fetched feed to items.
	newItems := processFeedItems(feed)

	// Merge new items with existing items from the last 24 hours.
	mergedItems, mergeErr := mergeFeedItemsAtParserLevel(feedURL, newItems)
	if mergeErr != nil {
		log.WithFields(logrus.Fields{
			"url":   feedURL,
			"error": mergeErr,
		}).Error("[fetchAndCacheFeed] Failed to merge feed items")
		return FeedResponse{}, mergeErr
	}

	baseDomain := getBaseDomain(feed.Link)
	addURLToList(feedURL)

	// Possibly fetch additional metadata from cache
	var metaData MetaDataResponseItem

	cacheMutex.Lock()
	defer cacheMutex.Unlock()

	baseDomainKey := createHash(baseDomain)
	if err := cache.Get(metaData_prefix, baseDomainKey, &metaData); err != nil {
		if isValidURL(baseDomain) {
			tempMeta, errGet := GetMetaData(baseDomain)
			if errGet != nil {
				log.WithFields(logrus.Fields{
					"baseDomain": baseDomain,
					"error":      errGet,
				}).Warn("[fetchAndCacheFeed] Failed to get metadata")
			} else {
				metaData = tempMeta
				if errSet := cache.Set(metaData_prefix, baseDomainKey, metaData, 24*time.Hour); errSet != nil {
					log.WithFields(logrus.Fields{
						"baseDomain": baseDomain,
						"error":      errSet,
					}).Error("[fetchAndCacheFeed] Failed to cache metadata")
				}
			}
		} else {
			metaData = MetaDataResponseItem{}
			log.WithFields(logrus.Fields{
				"baseDomain": baseDomain,
			}).Warn("[fetchAndCacheFeed] Invalid baseDomain")
		}
	} else {
		log.WithFields(logrus.Fields{
			"baseDomain": baseDomain,
		}).Debug("[fetchAndCacheFeed] Loaded metadata from cache")
	}

	// Build final FeedResponse from the merged items
	finalFeedResponse := createFeedResponse(feed, feedURL, metaData, mergedItems)

	// Cache the final feed response
	if err := cache.Set(feed_prefix, cacheKey, finalFeedResponse, 24*time.Hour); err != nil {
		log.WithFields(logrus.Fields{
			"url":   feedURL,
			"error": err,
		}).Error("[fetchAndCacheFeed] Failed to cache feed details")
		return FeedResponse{}, err
	}

	log.WithFields(logrus.Fields{
		"url": feedURL,
	}).Info("[fetchAndCacheFeed] Successfully cached feed details")
	return finalFeedResponse, nil
}

/**
 * @function processURL
 * @description Checks the cache for a feed URL; if found and not stale, returns the cached feed.
 *              Otherwise, calls fetchAndCacheFeed to retrieve and cache a fresh feed. Pagination is then applied
 *              to the final list of items before returning.
 * @param {string} rawURL The raw URL of the feed to process.
 * @param {int} page The page number for pagination.
 * @param {int} itemsPerPage The number of items per page for pagination.
 * @returns {FeedResponse} The processed FeedResponse, with pagination applied.
 * @dependencies sanitizeURL, cache, isCacheStale, fetchAndCacheFeed, updateFeedItemsWithThumbnailColors,
 *               applyPagination, log
 */
func processURL(rawURL string, page, itemsPerPage int) FeedResponse {
	feedURL := sanitizeURL(rawURL)
	cacheKey := feedURL

	var cachedFeed FeedResponse
	// Try retrieving from cache first
	if err := cache.Get(feed_prefix, cacheKey, &cachedFeed); err == nil && cachedFeed.SiteTitle != "" {
		// Cache hit
		log.WithFields(logrus.Fields{
			"url": feedURL,
		}).Info("[processURL] [Cache Hit] Using cached feed details")

		// Check staleness
		if isCacheStale(cachedFeed.LastRefreshed) {
			log.WithFields(logrus.Fields{
				"url": feedURL,
			}).Info("[processURL] Cache is stale, refreshing in background")
			go func() {
				if _, errRefresh := fetchAndCacheFeed(feedURL, cacheKey); errRefresh != nil {
					log.WithFields(logrus.Fields{
						"url":   feedURL,
						"error": errRefresh,
					}).Error("[processURL] Failed to refresh feed in background")
				}
			}()
		}

		// Optionally re-check or skip thumbnail colors
		updatedItems := updateFeedItemsWithThumbnailColors(cachedFeed.Items)
		// Reassign updated items
		cachedFeed.Items = &updatedItems

		// **Apply pagination** to the final items
		applyPagination(cachedFeed.Items, page, itemsPerPage)

		return cachedFeed
	}

	// Cache miss or empty feed
	log.WithFields(logrus.Fields{
		"url": feedURL,
	}).Info("[processURL] [Cache Miss] Fetching fresh feed")

	newResp, errNew := fetchAndCacheFeed(feedURL, cacheKey)
	if errNew != nil {
		log.WithFields(logrus.Fields{
			"url":   feedURL,
			"error": errNew,
		}).Error("[processURL] Failed to fetch and cache feed")

		return FeedResponse{
			Type:    "unknown",
			FeedUrl: feedURL,
			GUID:    cacheKey,
			Status:  "error",
			Error:   errNew,
		}
	}

	// Optionally re-check or skip thumbnail colors
	updatedItems := updateFeedItemsWithThumbnailColors(newResp.Items)
	newResp.Items = &updatedItems

	// **Apply pagination** to the final items
	applyPagination(newResp.Items, page, itemsPerPage)

	return newResp
}

/**
 * @function applyPagination
 * @description Modifies the feed items in place, slicing to the requested page and itemsPerPage
 *              (e.g. page=2, itemsPerPage=10 => skip first 10 items, return next 10).
 * @param {*[]FeedResponseItem} items A pointer to a slice of FeedResponseItem objects.
 * @param {int} page The page number to retrieve.
 * @param {int} itemsPerPage The number of items per page.
 * @returns {void}
 */
func applyPagination(items *[]FeedResponseItem, page, itemsPerPage int) {
	if items == nil || len(*items) == 0 {
		return
	}
	if page < 1 {
		page = 1
	}
	if itemsPerPage < 1 {
		itemsPerPage = 1
	}

	totalItems := len(*items)
	start := (page - 1) * itemsPerPage
	if start >= totalItems {
		// If start is beyond total items, return empty
		*items = []FeedResponseItem{}
		return
	}

	end := start + itemsPerPage
	if end > totalItems {
		end = totalItems
	}

	// Slice the items
	*items = (*items)[start:end]
}

/**
 * @function updateFeedItemsWithThumbnailColors
 * @description Iterates over existing items in a feed,
 *              calling updateThumbnailColorForItem to finalize or skip color checks.
 * @param {*[]FeedResponseItem} items A pointer to a slice of FeedResponseItem objects.
 * @returns {[]FeedResponseItem} A new slice of FeedResponseItem objects with updated thumbnail colors.
 * @dependencies updateThumbnailColorForItem
 */
func updateFeedItemsWithThumbnailColors(items *[]FeedResponseItem) []FeedResponseItem {
	if items == nil {
		return nil
	}
	var updatedItems []FeedResponseItem
	for _, item := range *items {
		updatedItem := updateThumbnailColorForItem(item)
		updatedItems = append(updatedItems, updatedItem)
	}
	return updatedItems
}

/**
 * @function updateThumbnailColorForItem
 * @description Checks if we have a cached color for the item’s thumbnail.
 *              If so, sets item.ThumbnailColor. Otherwise, logs that color is not yet available.
 * @param {FeedResponseItem} item The FeedResponseItem to update.
 * @returns {FeedResponseItem} The updated FeedResponseItem.
 * @dependencies cache, log
 */
func updateThumbnailColorForItem(item FeedResponseItem) FeedResponseItem {
	var cachedColor RGBColor
	err := cache.Get(thumbnailColor_prefix, item.Thumbnail, &cachedColor)

	switch {
	case err != nil:
		log.WithFields(logrus.Fields{
			"thumbnail": item.Thumbnail,
		}).Debug("[updateThumbnailColorForItem] No cached color")
	case item.ThumbnailColorComputed == "set":
		// Already set
	case item.ThumbnailColorComputed == "computed":
		item.ThumbnailColor = cachedColor
		item.ThumbnailColorComputed = "set"
		log.WithFields(logrus.Fields{
			"thumbnail": item.Thumbnail,
			"color":     item.ThumbnailColor,
		}).Debug("[updateThumbnailColorForItem] Updated color")
	case item.ThumbnailColorComputed == "no":
		if cachedColor != (RGBColor{}) {
			item.ThumbnailColor = cachedColor
			item.ThumbnailColorComputed = "set"
			log.WithFields(logrus.Fields{
				"thumbnail": item.Thumbnail,
				"color":     item.ThumbnailColor,
			}).Debug("[updateThumbnailColorForItem] Updated color")
		}
	default:
		// No additional logic
	}
	return item
}

/**
 * @function processFeedItems
 * @description Validates feed, concurrency processes each item, returning a slice of FeedResponseItem.
 * @param {*gofeed.Feed} feed The parsed feed to process.
 * @returns {[]FeedResponseItem} A slice of processed FeedResponseItem objects.
 * @dependencies processFeedItem, extractColorFromThumbnail_prominentColor, log
 */
func processFeedItems(feed *gofeed.Feed) []FeedResponseItem {
	// Safeguard feed == nil or feed.Items is nil/empty
	if feed == nil {
		log.Error("[processFeedItems] feed is nil; returning empty slice")
		return nil
	}
	if len(feed.Items) == 0 {
		log.WithFields(logrus.Fields{
			"feedTitle": feed.Title,
		}).Warn("[processFeedItems] feed.Items is empty")
		return nil
	}

	thumbnail := ""
	defaultThumbnailColor := RGBColor{128, 128, 128}
	// If iTunes image is present, compute default color
	if feed.ITunesExt != nil && feed.ITunesExt.Image != "" {
		thumbnail = feed.ITunesExt.Image
		r, g, b := extractColorFromThumbnail_prominentColor(thumbnail)
		defaultThumbnailColor = RGBColor{r, g, b}
	}

	var wg sync.WaitGroup
	sem := make(chan struct{}, numWorkers)
	itemResponses := make(chan FeedResponseItem, len(feed.Items))

	for _, item := range feed.Items {
		if item == nil {
			log.Warn("[processFeedItems] Skipping nil item")
			continue
		}
		wg.Add(1)
		sem <- struct{}{}
		go func(it *gofeed.Item) {
			defer func() {
				wg.Done()
				<-sem
			}()
			itemResponse := processFeedItem(it, thumbnail, defaultThumbnailColor)
			itemResponses <- itemResponse
		}(item)
	}

	go func() {
		wg.Wait()
		close(itemResponses)
	}()

	return collectItemResponses(itemResponses)
}

/**
 * @function processFeedItem
 * @description Creates a FeedResponseItem from a single gofeed.Item,
 *              attempting to discover a thumbnail if not set, and sets a default or cached color.
 * @param {*gofeed.Item} item The gofeed.Item to process.
 * @param {string} thumbnail The default thumbnail URL for the feed.
 * @param {RGBColor} thumbnailColor The default thumbnail color for the feed.
 * @returns {FeedResponseItem} The processed FeedResponseItem.
 * @dependencies getItemAuthor, NewThumbnailFinder, FindThumbnailForItem, extractColorFromThumbnail_prominentColor,
 *               parseHTMLContent, standardizeDate, determineItemTypeAndDuration, createHash, cache, log
 */
func processFeedItem(item *gofeed.Item, thumbnail string, thumbnailColor RGBColor) FeedResponseItem {
	author := getItemAuthor(item)
	categories := strings.Join(item.Categories, ", ")

	// Possibly override the feed-level thumbnail with item enclosures
	if len(item.Enclosures) > 0 {
		for _, enclosure := range item.Enclosures {
			if enclosure.URL != "" && strings.HasPrefix(enclosure.Type, "image/") {
				thumbnail = enclosure.URL
				break
			}
		}
	}
	if thumbnail == "" && item.Image != nil && item.Image.URL != "" {
		thumbnail = item.Image.URL
	}
	if item.ITunesExt != nil && item.ITunesExt.Image != "" {
		thumbnail = item.ITunesExt.Image
	}

	// Attempt to discover a thumbnail from content if still empty
	if thumbnail == "" {
		finder := NewThumbnailFinder()
		discovered := finder.FindThumbnailForItem(item)
		if discovered != "" {
			thumbnail = discovered
		}
	}

	thumbnailColorComputed := "no"
	// If thumbnail color is cached, set it directly; otherwise compute async
	var cachedColor RGBColor
	cacheMutex.Lock()
	err := cache.Get(thumbnailColor_prefix, thumbnail, &cachedColor)
	cacheMutex.Unlock()
	if err == nil {
		thumbnailColor = cachedColor
		thumbnailColorComputed = "set"
	} else {
		// Async compute if not cached
		go func(thURL string) {
			if thURL == "" {
				return
			}
			r, g, b := extractColorFromThumbnail_prominentColor(thURL)
			actualColor := RGBColor{r, g, b}
			if cErr := cache.Set(thumbnailColor_prefix, thURL, actualColor, 24*time.Hour); cErr != nil {
				log.WithFields(logrus.Fields{
					"thumbnail": thURL,
					"color":     actualColor,
					"error":     cErr,
				}).Error("[processFeedItem] Failed to cache color")
			}
		}(thumbnail)
	}

	desc := item.Description
	if desc == "" {
		desc = parseHTMLContent(item.Content)
	}
	desc = parseHTMLContent(desc)

	// Standardize item published date
	standardizedPublished := standardizeDate(item.Published)

	// Identify if it's a podcast and parse duration if so
	itemType, duration := determineItemTypeAndDuration(item)

	return FeedResponseItem{
		Type:                   itemType,
		ID:                     createHash(item.Link),
		Title:                  item.Title,
		Description:            desc,
		Link:                   item.Link,
		Duration:               duration,
		Author:                 author,
		Published:              standardizedPublished,
		Created:                standardizedPublished,
		Content:                parseHTMLContent(item.Content),
		Content_Encoded:        item.Content,
		Categories:             categories,
		Enclosures:             item.Enclosures,
		Thumbnail:              thumbnail,
		ThumbnailColor:         thumbnailColor,
		ThumbnailColorComputed: thumbnailColorComputed,
	}
}

/**
 * @function standardizeDate
 * @description Parses dateStr in various known formats (RFC3339, RFC1123, etc.).
 *              Returns the date in a standard layout or empty if parse fails.
 * @param {string} dateStr The date string to parse.
 * @returns {string} The standardized date string, or an empty string if parsing fails.
 * @dependencies time.Parse, time.RFC1123, time.RFC1123Z, time.RFC3339, time.RFC822, time.RFC850, time.ANSIC, log
 */
func standardizeDate(dateStr string) string {
	if dateStr == "" {
		log.Info("[standardizeDate] Empty date string")
		return ""
	}
	const outputLayout = "2006-01-02T15:04:05Z07:00"
	dateFormats := []string{
		time.RFC1123,
		time.RFC1123Z,
		time.RFC3339,
		time.RFC822,
		time.RFC850,
		time.ANSIC,
		"Mon, 02 Jan 2006 15:04:05 -0700",
	}
	for _, layout := range dateFormats {
		if parsedTime, err := time.Parse(layout, dateStr); err == nil {
			return parsedTime.Format(outputLayout)
		}
	}
	log.WithFields(logrus.Fields{
		"date": dateStr,
	}).Info("[standardizeDate] Failed to parse date")
	return ""
}

/**
 * @function createFeedResponse
 * @description Builds a FeedResponse struct from a parsed feed object, feed metadata, and items.
 * @param {*gofeed.Feed} feed The parsed feed.
 * @param {string} feedURL The URL of the feed.
 * @param {MetaDataResponseItem} metaData The metadata for the feed.
 * @param {[]FeedResponseItem} feedItems The processed feed items.
 * @returns {FeedResponse} The constructed FeedResponse object.
 * @dependencies createHash, log
 */
func createFeedResponse(feed *gofeed.Feed, feedURL string, metaData MetaDataResponseItem, feedItems []FeedResponseItem) FeedResponse {
	if feed == nil {
		log.WithFields(logrus.Fields{
			"url": feedURL,
		}).Error("[createFeedResponse] feed is nil")
		return FeedResponse{}
	}

	var feedType, thumbnail string
	if feed.ITunesExt != nil {
		feedType = "podcast"
		if feed.Image != nil && feed.Image.URL != "" {
			thumbnail = feed.Image.URL
		}
	} else {
		feedType = "article"
		if metaData.Favicon != "" {
			thumbnail = metaData.Favicon
		} else if feed.Image != nil && feed.Image.URL != "" {
			thumbnail = feed.Image.URL
		}
	}

	siteTitle := metaData.Sitename
	if siteTitle == "" {
		siteTitle = feed.Title
	}

	return FeedResponse{
		Status:        "ok",
		GUID:          createHash(feedURL),
		Type:          feedType,
		SiteTitle:     siteTitle,
		FeedTitle:     feed.Title,
		FeedUrl:       feedURL,
		Description:   feed.Description,
		Link:          metaData.Domain,
		LastUpdated:   standardizeDate(feed.Updated),
		LastRefreshed: time.Now().Format(layout),
		Published:     feed.Published,
		Author:        feed.Author,
		Language:      feed.Language,
		Favicon:       thumbnail,
		Categories:    strings.Join(feed.Categories, ", "),
		Items:         &feedItems,
	}
}

/**
 * @function collectResponses
 * @description Reads FeedResponse objects from a channel, returning them as a slice.
 * @param {chan FeedResponse} responses The channel to read from.
 * @returns {[]FeedResponse} A slice of FeedResponse objects.
 */
func collectResponses(responses chan FeedResponse) []FeedResponse {
	var all []FeedResponse
	for resp := range responses {
		all = append(all, resp)
	}
	return all
}

/**
 * @function collectItemResponses
 * @description Reads FeedResponseItem objects from a channel, returning them as a slice.
 *              The items are then sorted by descending Published date.
 * @param {chan FeedResponseItem} itemResponses The channel to read from.
 * @returns {[]FeedResponseItem} A slice of FeedResponseItem objects, sorted by descending Published date.
 * @dependencies time.Parse, log
 */
func collectItemResponses(itemResponses chan FeedResponseItem) []FeedResponseItem {
	var feedItems []FeedResponseItem
	for itemResponse := range itemResponses {
		feedItems = append(feedItems, itemResponse)
	}
	// Sort by published date (descending)
	sort.Slice(feedItems, func(i, j int) bool {
		timeI, errI := time.Parse(layout, feedItems[i].Published)
		if errI != nil {
			log.WithFields(logrus.Fields{
				"item":  feedItems[i],
				"error": errI,
			}).Error("[collectItemResponses] Failed to parse time for item I")
			return false
		}
		timeJ, errJ := time.Parse(layout, feedItems[j].Published)
		if errJ != nil {
			log.WithFields(logrus.Fields{
				"item":  feedItems[j],
				"error": errJ,
			}).Error("[collectItemResponses] Failed to parse time for item J")
			return true
		}
		return timeI.After(timeJ)
	})
	return feedItems
}

/**
 * @function sendResponse
 * @description Writes a JSON-encoded Feeds struct to the ResponseWriter.
 * @param {http.ResponseWriter} w The HTTP response writer.
 * @param {[]FeedResponse} responses The FeedResponse objects to encode and send.
 * @dependencies json.NewEncoder, log
 */
func sendResponse(w http.ResponseWriter, responses []FeedResponse) {
	w.Header().Set("Content-Type", "application/json")
	enc := json.NewEncoder(w)

	feeds := Feeds{Feeds: responses}
	if err := enc.Encode(feeds); err != nil {
		log.WithFields(logrus.Fields{
			"error": err,
		}).Error("[sendResponse] Failed to encode feed responses")
	}
}

/**
 * @function refreshFeeds
 * @description Retrieves all known feed URLs from the cache and re-processes them
 *              (useful for cron-based or ticker-based refreshing).
 * @dependencies getAllCachedURLs, processURL, log
 */
func refreshFeeds() {
	urls := getAllCachedURLs()
	for _, url := range urls {
		log.WithFields(logrus.Fields{
			"url": url,
		}).Info("[refreshFeeds] Refreshing feed")
		_ = processURL(url, 1, 20) // or any default paging
	}
}

/**
 * @function addURLToList
 * @description Ensures the feed URL is tracked in urlList (the list of subscribed or known feeds).
 * @param {string} url The URL to add to the list.
 * @dependencies urlListMutex, stringInSlice
 */
func addURLToList(url string) {
	urlListMutex.Lock()
	defer urlListMutex.Unlock()

	if !stringInSlice(url, urlList) {
		urlList = append(urlList, url)
	}
}

/**
 * @function getAllCachedURLs
 * @description Returns all known feed URLs from the cache or from an in-memory list.
 *              If none are found, returns an empty slice.
 * @returns {[]string} A slice of known feed URLs.
 * @dependencies urlListMutex, cache.GetSubscribedListsFromCache, log
 */
func getAllCachedURLs() []string {
	urlListMutex.Lock()
	defer urlListMutex.Unlock()

	if len(urlList) == 0 {
		startTime := time.Now()

		var err error
		urlList, err = cache.GetSubscribedListsFromCache(feed_prefix)
		if err != nil {
			log.WithFields(logrus.Fields{
				"error": err,
			}).Warn("[getAllCachedURLs] Failed to get subscribed feeds from cache")
			return nil
		}

		duration := time.Since(startTime)
		log.WithFields(logrus.Fields{
			"urlList":  urlList,
			"duration": duration,
		}).Info("[getAllCachedURLs] Loaded urlList from cache")
	}

	// return a copy
	return append([]string(nil), urlList...)
}

/**
 * @function isValidURL
 * @description Checks if str is a syntactically valid URL with a resolvable host or IP.
 * @param {string} str The string to check.
 * @returns {bool} True if str is a valid URL, false otherwise.
 * @dependencies url.ParseRequestURI, net.ParseIP, strings.Contains
 */
func isValidURL(str string) bool {
	parsedURL, err := url.ParseRequestURI(str)
	if err != nil {
		log.WithFields(logrus.Fields{
			"url":   str,
			"error": err,
		}).Info("[isValidURL] Invalid URL")
		return false
	}
	host := parsedURL.Hostname()
	if net.ParseIP(host) != nil {
		return true
	}
	return strings.Contains(host, ".")
}

/**
 * @function sanitizeURL
 * @description Ensures URLs always use https:// if no scheme or if http://
 * @param {string} rawURL The raw URL to sanitize.
 * @returns {string} The sanitized URL.
 * @dependencies url.Parse
 */
func sanitizeURL(rawURL string) string {
	parsedURL, err := url.Parse(rawURL)
	if err != nil || parsedURL.Scheme == "" {
		return "https://" + rawURL
	} else if parsedURL.Scheme == "http" {
		return strings.Replace(rawURL, "http://", "https://", 1)
	}
	return rawURL
}

/**
 * @function getItemAuthor
 * @description Returns an item’s author if set, checking iTunesExt first (for podcasts).
 * @param {*gofeed.Item} item The feed item.
 * @returns {string} The author of the item, or an empty string if not found.
 */
func getItemAuthor(item *gofeed.Item) string {
	if item.ITunesExt != nil && item.ITunesExt.Author != "" {
		return item.ITunesExt.Author
	}
	if item.Author != nil && item.Author.Name != "" {
		return item.Author.Name
	}
	return ""
}

/**
 * @function determineItemTypeAndDuration
 * @description Checks if the feed item is a podcast. If yes,
 *              parses the item’s duration. Returns a type (podcast or article) and the duration in seconds.
 * @param {*gofeed.Item} item The feed item.
 * @returns {string, int} The type of the item ("podcast" or "article") and the duration in seconds.
 * @dependencies parseDuration
 */
func determineItemTypeAndDuration(item *gofeed.Item) (string, int) {
	if item.ITunesExt != nil {
		return "podcast", parseDuration(item.ITunesExt.Duration)
	}
	return "article", 0
}

/**
 * @function parseDuration
 * @description Attempts to convert a time-like string (e.g., 3600 or HH:MM:SS) into total seconds.
 * @param {string} durationStr The duration string to parse.
 * @returns {int} The duration in seconds, or 0 if parsing fails.
 * @dependencies strconv.Atoi, strings.Split
 */
func parseDuration(durationStr string) int {
	if durationStr == "" {
		return 0
	}

	// Try integer first
	if durationInt, err := strconv.Atoi(durationStr); err == nil {
		return durationInt
	}

	// Possibly HH:MM:SS or MM:SS
	parts := strings.Split(durationStr, ":")
	switch len(parts) {
	case 3:
		hours, _ := strconv.Atoi(parts[0])
		minutes, _ := strconv.Atoi(parts[1])
		seconds, _ := strconv.Atoi(parts[2])
		return hours*3600 + minutes*60 + seconds
	case 2:
		minutes, _ := strconv.Atoi(parts[0])
		seconds, _ := strconv.Atoi(parts[1])
		return minutes*60 + seconds
	default:
		return 0
	}
}

/**
 * @function stringInSlice
 * @description Returns true if str is found in the slice list.
 * @param {string} str The string to search for.
 * @param {[]string} list The slice to search in.
 * @returns {bool} True if str is found in list, false otherwise.
 */
func stringInSlice(str string, list []string) bool {
	for _, v := range list {
		if v == str {
			return true
		}
	}
	return false
}

/**
 * @function mergeFeedItemsAtParserLevel
 * @description Merges old items from the cache with newly fetched items, deduplicating by ID
 *              and retaining only items from within the last 24 hours (cachePeriod). Also updates items if content changed.
 * @param {string} feedURL The URL of the feed.
 * @param {[]FeedResponseItem} newItems The newly fetched feed items.
 * @returns {[]FeedResponseItem, error} The merged slice of FeedResponseItem objects, or an error if retrieval from cache fails.
 * @dependencies cache.Get, isWithinPeriod, isUpdatedContent, log
 */
func mergeFeedItemsAtParserLevel(feedURL string, newItems []FeedResponseItem) ([]FeedResponseItem, error) {
	cacheKey := feedURL
	var existingFeedResponse FeedResponse
	var existingItems []FeedResponseItem

	// Attempt to get an existing feed from the cache
	if err := cache.Get(feed_prefix, cacheKey, &existingFeedResponse); err != nil {
		log.WithFields(logrus.Fields{
			"url":   feedURL,
			"error": err,
		}).Error("[mergeFeedItemsAtParserLevel] Error getting existing items")
		existingItems = nil
	} else {
		if existingFeedResponse.Items != nil {
			existingItems = *existingFeedResponse.Items
			log.WithFields(logrus.Fields{
				"url":   feedURL,
				"count": len(existingItems),
			}).Debug("[mergeFeedItemsAtParserLevel] Found existing items")
		}
	}

	itemMap := make(map[string]FeedResponseItem)
	log.WithFields(logrus.Fields{
		"url":      feedURL,
		"existing": len(existingItems),
		"new":      len(newItems),
	}).Debug("[mergeFeedItemsAtParserLevel] Merging items")
	for _, oldItem := range existingItems {
		if isWithinPeriod(oldItem, cachePeriod) {
			itemMap[oldItem.ID] = oldItem
		}
	}

	// Merge new items
	for _, newIt := range newItems {
		if oldIt, found := itemMap[newIt.ID]; found {
			if isUpdatedContent(oldIt, newIt) {
				itemMap[newIt.ID] = newIt
			}
		} else {
			if isWithinPeriod(newIt, cachePeriod) {
				itemMap[newIt.ID] = newIt
			}
		}
	}

	merged := make([]FeedResponseItem, 0, len(itemMap))
	for _, v := range itemMap {
		merged = append(merged, v)
	}

	// Store merged items in the cache so subsequent fetches have updated items
	if err := cache.Set(feed_prefix, feedURL, merged, 24*time.Hour); err != nil {
		log.WithFields(logrus.Fields{
			"url":   feedURL,
			"error": err,
		}).Error("[mergeFeedItemsAtParserLevel] Failed to cache merged items")
		return nil, err
	}

	return merged, nil
}

/**
 * @function isWithinPeriod
 * @description Returns true if the FeedResponseItem’s Published date is within 'days' days of now.
 * @param {FeedResponseItem} item The FeedResponseItem to check.
 * @param {int} days The number of days to check within.
 * @returns {bool} True if the item's Published date is within the specified period, false otherwise.
 * @dependencies time.Parse, time.Since
 */
func isWithinPeriod(item FeedResponseItem, days int) bool {
	t, err := time.Parse(layout, item.Published)
	if err != nil {
		return false
	}
	return time.Since(t) <= time.Duration(days)*24*time.Hour
}

/**
 * @function isUpdatedContent
 * @description Returns true if newIt is more recent than oldIt by published date
 *              or if newIt’s content differs from oldIt’s content.
 * @param {FeedResponseItem} oldIt The old FeedResponseItem.
 * @param {FeedResponseItem} newIt The new FeedResponseItem.
 * @returns {bool} True if the new item is more recent or has different content, false otherwise.
 * @dependencies time.Parse
 */
func isUpdatedContent(oldIt, newIt FeedResponseItem) bool {
	oldTime, _ := time.Parse(layout, oldIt.Published)
	newTime, _ := time.Parse(layout, newIt.Published)

	// If new item is more recent
	if newTime.After(oldTime) {
		return true
	}
	// If content changed
	if newIt.Content != oldIt.Content {
		return true
	}
	return false
}

//routes.go
// Package main provides the main functionality for the web server.
package main

import (
	"net/http"

	"github.com/sirupsen/logrus"
)

/**
 * @function errorMiddlewareFunc
 * @description Middleware function that recovers from panics, logs the error, and sends a 500 response.
 * @param {http.HandlerFunc} next The next handler function in the chain.
 * @returns {http.HandlerFunc} The wrapped handler function.
 * @dependencies log
 */
func errorMiddlewareFunc(next http.HandlerFunc) http.HandlerFunc {
	return func(w http.ResponseWriter, r *http.Request) {
		defer func() {
			if err := recover(); err != nil {
				log.WithFields(logrus.Fields{
					"error": err,
				}).Error("An error occurred")
				http.Error(w, "Internal Server Error", http.StatusInternalServerError)
			}
		}()
		next(w, r)
	}
}

/**
 * @function InitializeRoutes
 * @description Sets up all the routes for the application.
 * @param {*http.ServeMux} mux The HTTP request multiplexer.
 * @returns {void}
 * @dependencies errorMiddlewareFunc, validateURLsHandler, parseHandler, discoverHandler,
 *               getReaderViewHandler, createShareHandler, shareHandler, searchHandler,
 *               streamAudioHandler, metadataHandler
 */
func InitializeRoutes(mux *http.ServeMux) {
	mux.HandleFunc("/validate", errorMiddlewareFunc(validateURLsHandler))
	mux.HandleFunc("/parse", errorMiddlewareFunc(parseHandler))
	mux.HandleFunc("/discover", errorMiddlewareFunc(discoverHandler))
	mux.HandleFunc("/getreaderview", errorMiddlewareFunc(getReaderViewHandler))
	mux.HandleFunc("/create", errorMiddlewareFunc(createShareHandler))
	mux.HandleFunc("/share", errorMiddlewareFunc(shareHandler))
	mux.HandleFunc("/search", errorMiddlewareFunc(searchHandler))
	mux.HandleFunc("/streamaudio", errorMiddlewareFunc(streamAudioHandler))
	mux.HandleFunc("/metadata", errorMiddlewareFunc(metadataHandler))
}

//streamaudio.go
// Package main provides the main functionality for the web server.
package main

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"net/http"
	"strings"
	"sync"
	"time"

	texttospeech "cloud.google.com/go/texttospeech/apiv1"
	"cloud.google.com/go/texttospeech/apiv1/texttospeechpb"
	"github.com/sirupsen/logrus"
)

var (
	ttsClient *texttospeech.Client
	once      sync.Once
)

/**
 * @function initTTSClient
 * @description Initializes the Google Cloud Text-to-Speech client.
 * @returns {void}
 * @dependencies texttospeech.NewClient, log
 */
func initTTSClient() {
	var err error
	ttsClient, err = texttospeech.NewClient(context.Background())
	if err != nil {
		log.WithFields(logrus.Fields{
			"error": err,
		}).Fatal("Failed to create TTS client")
	}
}

/**
 * @function splitTextIntoChunks
 * @description Splits a text into chunks of a specified maximum size,
 *              ensuring that words are not split across chunks.
 * @param {string} text The text to split.
 * @param {int} maxChunkSize The maximum size of each chunk.
 * @returns {[]string} A slice of text chunks.
 */
func splitTextIntoChunks(text string, maxChunkSize int) []string {
	var chunks []string
	words := strings.Fields(text)
	var chunk string

	for _, word := range words {
		if len(chunk)+len(word)+1 > maxChunkSize {
			chunks = append(chunks, chunk)
			chunk = word
		} else {
			if chunk != "" {
				chunk += " "
			}
			chunk += word
		}
	}
	if chunk != "" {
		chunks = append(chunks, chunk)
	}

	return chunks
}

/**
 * @function streamAudioHandler
 * @description Handles HTTP requests to the /streamaudio endpoint.
 *              It expects a POST request with a JSON body containing text or a URL to synthesize.
 *              It synthesizes the text to speech using the Google Cloud Text-to-Speech API,
 *              caches the audio content, and streams it to the client.
 * @param {http.ResponseWriter} w The HTTP response writer.
 * @param {*http.Request} r The HTTP request.
 * @returns {void}
 * @dependencies cache, log, once, initTTSClient, ttsClient, splitTextIntoChunks
 */
func streamAudioHandler(w http.ResponseWriter, r *http.Request) {
	log.Info("Received request to stream audio")

	// Ensure it's a POST request
	if r.Method != http.MethodPost {
		log.WithFields(logrus.Fields{
			"method": r.Method,
		}).Warn("[streamAudioHandler] Invalid method")
		http.Error(w, "Only POST method is allowed", http.StatusMethodNotAllowed)
		return
	}

	// Read and parse the request body
	var ttsReq TTSRequest
	err := json.NewDecoder(r.Body).Decode(&ttsReq)
	if err != nil {
		log.WithFields(logrus.Fields{
			"error": err,
		}).Error("[streamAudioHandler] Error decoding request body")
		http.Error(w, "Bad request: "+err.Error(), http.StatusBadRequest)
		return
	}

	log.WithFields(logrus.Fields{
		"text": ttsReq.Text,
		"url":  ttsReq.Url,
	}).Debug("[streamAudioHandler] Request received")

	// Check if text is provided
	if ttsReq.Text == "" {
		log.Warn("[streamAudioHandler] No text provided")
		http.Error(w, "No text provided", http.StatusBadRequest)
		return
	} else if ttsReq.Url != "" {
		// Check if the URL is valid
		if !(strings.HasPrefix(ttsReq.Url, "http://") || strings.HasPrefix(ttsReq.Url, "https://")) {
			log.WithFields(logrus.Fields{
				"url": ttsReq.Url,
			}).Warn("[streamAudioHandler] Invalid URL provided")
			http.Error(w, "Invalid URL provided", http.StatusBadRequest)
			return
		}
	}

	cacheKey := ttsReq.Url
	if cacheKey == "" {
		cacheKey = createHash(ttsReq.Text)
	}

	var cachedAudio []byte
	// Check if the audio content is cached
	err = cache.Get(audio_prefix, cacheKey, &cachedAudio)
	if err == nil {
		log.WithFields(logrus.Fields{
			"key": cacheKey,
		}).Debug("[streamAudioHandler] Audio content found in cache")
		// Set the headers and write the audio content to the response
		w.Header().Set("Content-Type", "audio/mpeg")
		w.Header().Set("Content-Length", fmt.Sprint(len(cachedAudio)))

		// Write the audio content to the response
		_, err = w.Write(cachedAudio)
		if err != nil {
			log.WithFields(logrus.Fields{
				"error": err,
			}).Error("[streamAudioHandler] Failed to write audio content to response")
		}
		return
	}

	// Initialize the TTS client once
	once.Do(initTTSClient)

	log.WithFields(logrus.Fields{
		"text": ttsReq.Text,
	}).Debug("[streamAudioHandler] Text to be synthesized")
	const maxChunkSize = 1000

	// Split text into chunks of up to 1000 characters
	chunks := splitTextIntoChunks(ttsReq.Text, maxChunkSize)

	var audioContent bytes.Buffer

	for _, chunk := range chunks {
		req := texttospeechpb.SynthesizeSpeechRequest{
			// Set the text input to be synthesized.
			Input: &texttospeechpb.SynthesisInput{
				InputSource: &texttospeechpb.SynthesisInput_Text{Text: chunk},
			},
			// Build the voice request, select the language code ("en-US") and the SSML
			// voice gender ("neutral").
			Voice: &texttospeechpb.VoiceSelectionParams{
				LanguageCode: "en-US",
				Name:         "en-US-Neural2-J",
			},
			// Select the type of audio file you want returned.
			AudioConfig: &texttospeechpb.AudioConfig{
				AudioEncoding: *texttospeechpb.AudioEncoding_MP3.Enum(),
			},
		}
		// Perform the text-to-speech request
		resp, err := ttsClient.SynthesizeSpeech(context.Background(), &req)
		if err != nil {
			log.WithFields(logrus.Fields{
				"error": err,
			}).Error("[streamAudioHandler] Failed to synthesize speech")
			http.Error(w, "Failed to synthesize speech", http.StatusInternalServerError)
			return
		} else {
			log.Debug("[streamAudioHandler] Speech synthesized successfully")
		}

		// Append the audio content to the buffer
		audioContent.Write(resp.AudioContent)
	}

	// Cache the audio content
	if err := cache.Set(audio_prefix, cacheKey, audioContent.Bytes(), 7*24*time.Hour); err != nil {
		log.WithFields(logrus.Fields{
			"key":   cacheKey,
			"error": err,
		}).Error("[streamAudioHandler] Failed to cache audio content")
	} else {
		log.WithFields(logrus.Fields{
			"key": cacheKey,
		}).Debug("[streamAudioHandler] Audio content cached successfully")
	}

	// Set the headers and write the audio content to the response
	w.Header().Set("Content-Type", "audio/mpeg")
	w.Header().Set("Content-Length", fmt.Sprint(audioContent.Len()))

	// Write the audio content to the response
	_, err = w.Write(audioContent.Bytes())
	if err != nil {
		log.WithFields(logrus.Fields{
			"error": err,
		}).Error("[streamAudioHandler] Failed to write audio content to response")
	}
}

